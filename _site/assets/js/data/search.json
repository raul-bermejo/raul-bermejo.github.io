[
  
  {
    "title": "Nesher Bari - Data Preparation",
    "url": "/posts/nesher_bari-part3/",
    "categories": "portfolio",
    "tags": "ai/ml, data-science",
    "date": "2023-10-15 11:00:00 +1000",
    





    "snippet": "In the previous two parts of this series, we introduced the Nesher Bari project and dived into an Exploratory Data Analysis (EDA) of one of the main datasets.In this post, I will walk through how we can prepare and clean the dataset, so it is ready for ingestion by an AI/ML model.import librariesimport pandas as pdimport numpy as npimport osfrom functools import reducefrom IPython.display import display_htmlfrom itertools import chain,cyclepd.set_option('display.max_columns', 100)ChallengesThe main challenge we have to address is data inconsistency. That is, different datasets have different conventions for unique identifiers (id’s). To address these issues, we will have to transform the datasets using some logic:df_ornitela_raw = pd.read_csv('./../data/Ornitela_Vultures_Gyps_fulvus_TAU_UCLA_Israel_newer.csv')df_movebank_raw = pd.read_csv(\"./../data/eda_movebank_dataset.csv\")# make all ids upper case in ornitela dataframesdf_ornitela = (    df_ornitela_raw    .copy()    .dropna())print(f\"The raw Ornitela time series has {len(df_ornitela_raw)}, but {len(df_ornitela_raw)-len(df_ornitela)} are duplicates\")df_ornitela['individual-local-identifier'] = df_ornitela['individual-local-identifier'].str.upper() # transform 'white' stirng to just 'w' in whoswho and mortality dataframesflag_color = lambda x: x.lower().split(' ')[1] in ['white', 'black', 'b', 'w'] if len(x.split(' ')) &gt; 1 else Falseshorten_color = lambda x: x.split(\" \")[0]+x.split(\" \")[1][0]flag_separation = lambda x: lambda x: x.lower().split(' ')[1] in ['white', 'black', 'b', 'w'] if len(x.split(' ')) &gt; 1 else Falselist_updated_dfs = []for df_i_raw in [df_mortality_raw, df_whoswho_raw]:    df_i = df_i_raw.copy()    df_i = df_i[df_i['Movebank_id'].notna()]    df_i['Movebank_id'] = df_i['Movebank_id'].astype(str)    df_i['is_colorful'] = df_i['Movebank_id'].apply(flag_color)    df_i['Movebank_id'] = (        df_i        .apply(lambda row: shorten_color(row.Movebank_id) if row.is_colorful else row.Movebank_id, axis=1)    )    df_i['Movebank_id'] = df_i['Movebank_id'].str.upper()    df_i = df_i.drop('is_colorful', axis=1)    list_updated_dfs.append(df_i)df_mortality, df_whoswho = list_updated_dfsNow we have consistent keys that allow us to join these three datasets: the Ornitela dataset, the “Who’s who” dataset and the mortality dataset, the latter being used to extract the target data. Note that the Who’s who dataset has some strange tags e.g., 'S94&gt;A99W' and 'Y11&gt;T98W'. These tags might correspond to either multiple vultures or special tags, something that the rangers will be able to confirm.Next, we can explore how many overlapping vultures each of the last two has with respect to the Ornitela dataset:ids_dict = {    \"ornitela\": ids_ornitela,    \"whoswho\": ids_whoswho,    \"mortality\": ids_mortality}for dataset, arr in ids_dict.items():    if dataset == \"ornitela\":        print(f\"The ornitela dataset has {len(arr)} unique vultures.\")        print(\"=\"*100)    else:        arr_intersect = list(set(ids_ornitela) &amp; set(arr))        print(f\"The {dataset} dataset has {len(arr_intersect)} vulture in common with the Ornitela dataset:\")        print(arr_intersect)        print(\"-\"*80)print(\"=\"*100)The ornitela dataset has 110 unique vultures.====================================================================================================The whoswho dataset has 110 vulture in common with the Ornitela dataset:['J33W', 'Y26', 'A57W', 'A73W', 'E32W', 'E01W', 'A20W', 'J39W', 'E16W', 'E33W', 'A31W', 'A10W', 'E11W', 'T13B', 'A35W', 'T71W', 'E37W', 'A03W', 'A15W', 'E09W', 'T61W', 'T91B', 'J31W', 'T17W', 'J66W', 'A38W', 'T77W', 'E19W', 'J28W', 'T14W', 'J38W', 'J12W', 'Y26B', 'A22W', 'T59W', 'E45W', 'J15W', 'E36W', 'E03', 'A18W', 'J34W', 'E15W', 'A13W', 'T86B', 'A16W', 'A01W', 'J11W', 'T70W', 'J17W', 'J35W', 'A08W', 'E10W', 'A33W', 'A29W', 'T90B', 'T53B', 'A04W', 'E14W', 'J30W', 'A02W', 'E17W', 'A76W', 'A09W', 'E13W', 'E12W', 'E38W', 'E07W', 'A32W', 'E04W', 'E02W', 'T69B', 'J19W', 'T76W', 'T19B', 'A19W', 'E34W', 'A39W', 'A53W', 'E03W', 'J36W', 'A75W', 'T50B', 'J00W', 'T85W', 'A55W', 'E05W', 'T25B', 'A78W', 'J16W', 'A52W', 'J05W', 'A05W', 'E41W', 'E39W', 'A00W', 'A58W', 'T99B', 'T79W', 'J53W', 'T66W', 'T56B', 'E30W', 'Y27B', 'J18W', 'A36W', 'T15W', 'A56W', 'J06W', 'J32W', 'E00W']--------------------------------------------------------------------------------The mortality dataset has 27 vulture in common with the Ornitela dataset:['J28W', 'J36W', 'E14W', 'T85W', 'A20W', 'T59W', 'E33W', 'A76W', 'J15W', 'E03', 'A18W', 'E07W', 'T71W', 'J53W', 'T69B', 'T66W', 'T56B', 'J18W', 'T86B', 'E09W', 'J17W', 'T17W', 'E10W', 'J66W', 'A38W', 'T77W', 'E19W']--------------------------------------------------------------------------------====================================================================================================So we see that all the Ornitela vultures are there in the who’s who dataset, whereas only 27 Ornitela vultures are present in the mortality dataset. This discrepency is mainly due to the mortality dataset only having information about deceased vultures, whereas the who’s who dataset has information about alive vultures too.Who’s Who DatasetIn this section, we’ll explore how we can use the ‘Whos Who’ dataset for getting more insights of the vultures. There are approx. empty 150 columns, which are also present in the raw data. We will get rid of these and also examine for what vultures we have information on whether they’re alive or deceased:df_whoswho = df_whoswho.loc[:, ~df_whoswho.columns.str.contains('^Unnamed')]ORNITELA_STUDY_NAME = \"Ornitela_Vultures_Gyps_fulvus_TAU_UCLA_Israel\"df_whoswho = (    df_whoswho    .drop_duplicates(subset='Movebank_id')    .dropna(subset=[\"is_alive\"]))    df_whoswho_dead = (    df_whoswho[        (df_whoswho['is_alive'] == 0) &amp; (df_whoswho['date_death'].notna())    ])df_whoswho_ornitela_dead = (    df_whoswho[        (df_whoswho['is_alive'] == 0) &amp; (df_whoswho['date_death'].notna()) &amp;        (df_whoswho[\"Movebank_study\"] == ORNITELA_STUDY_NAME)    ])So in the who’s who dataset, we only have about 10% information about deceased Ornitela vultures. This corresponds to only 18 out of the 101 Ornitela vultures.In the tables above, we can see that for both all records and the deceased vulture subset, the majority of vultures correspond to the Gyps fulvus INPA Hatzofe study, which explains why we have mortality information about only a few vultures.However, we can still use the Who’s Who dataset as either a look up table with vulture information, but more importantly, to construct our train dataset. That is, we can use this information to tag dead or alive Ornitela vultures in the time series data (ornitela dataset) granted they’re missing in the mortality dataset. Let’s construct this dataframe that we will use to construct an ML trainable Ornitela dataset - we just need to select the relevant columns for tagging mortality and transform the date of death into a datetime datatype. I will also rename the column details_stop to reason:df_whoswho_ornitela = df_whoswho[df_whoswho['Movebank_study'] == ORNITELA_STUDY_NAME]cols_whoswho_clean = [    'Nili_id',    'Movebank_id',    #'is_alive',    'date_death',    'details_stop']df_whoswho_ornitela_clean = (    df_whoswho_ornitela[cols_whoswho_clean]    .reset_index(drop=True)    .rename(columns={'details_stop': 'reason'}))df_whoswho_ornitela_clean['date_death'] = pd.to_datetime(df_whoswho_ornitela_clean['date_death'],                                                         format='%Y-%m-%d', errors='coerce')df_whoswho_ornitela_cleanMortality DatasetTo start, let’s print an overview of the Mortality dataset (dead_and_injured_vultures.xlsx), and find out how many of the records correspond to death and injured vultures:print(f\"There are {len(df_mortality_raw)} vultures in the original Whos who dataset.\")print(f\"Out of those records, {len(df_mortality_raw.drop_duplicates(subset='Movebank_id'))} are unique records.\")df_mortality.tail()There are 88 vultures in the original Whos who dataset.Out of those records, 88 are unique records.Next, let’s find out how many common vultures this dataset has both with the Who’s Who and the ornitela table, similar to what we did for the Ornitela dataset:print(f\"The mortality dataset has {len(ids_mortality)} unique vultures.\")print(\"-\"*80)for dataset, arr in ids_dict.items():    if dataset == \"mortality\":        pass    else:        arr_intersect = list(set(ids_mortality) &amp; set(arr))        print(f\"The {dataset} dataset has {len(arr_intersect)} vultures in common with the mortality dataset:\")        print(arr_intersect)        print(\"-\"*80)arr_intersect_all = reduce(np.intersect1d, [ids_mortality, ids_ornitela, ids_whoswho])print(f\"The three datasets have {len(arr_intersect_all)} vultures in common, mainly:\")        print(arr_intersect_all)print(\"-\"*80)print(    f\"Are the common vultures between the three datasets the same as the common vultures between the who's who and mortality datasets?\",    set(arr_intersect) == set(np.intersect1d(ids_mortality, ids_whoswho)))print(\"=\"*100)By running this we find a few insights:  Every vulture that’s present in the mortality table is also present in the who’s who dataset: this indicates that the mortality dataset also contains information  The Ornitela and mortality datasets have 27 vultures in common, and the three datasets also have 27 vultures in commonNow let’s examine the Ornitela vultures in the mortality dataset:df_mortality_ornitela = df_mortality[df_mortality['Movebank_id'].isin(df_ornitela['individual-local-identifier'].unique())].reset_index(drop=True)df_mortality_ornitelaWhen running this we can see that only two Ornitela vultures are injured in the mortality dataset. For the sake of simplicity and because we’re only considered alive or dead as a target flag, we’ll consider those two records to be alive. That way we can rename the column death or injury date to date_death and set the value of this column and reason to a NaT and NaN respectively for those injured vultures.After that, we can trim a few columns that won’t be neccesary to get a clean mortality dataset that we will also use to make the ML trainable Ornitela dataset and compare it to the Who’s who cleaned dataset above:ids_injured = [\"J17W\", \"T71W\"]for id_i in ids_injured:    df_mortality_ornitela.loc[df_mortality_ornitela['Movebank_id'] == id_i, 'death or injury date'] = pd.NaT    df_mortality_ornitela.loc[df_mortality_ornitela['Movebank_id'] == id_i, 'reason'] = np.NaNcols_mortality_clean = [    'Nili_id',     'Movebank_id',    'death or injury date',#    'fate',     'reason', ]df_mortality_ornitela_clean = (    df_mortality_ornitela[cols_mortality_clean]    .reset_index(drop=True)    .rename(columns={'death or injury date': 'date_death'}))display(df_mortality_ornitela_clean)display(df_whoswho_ornitela_clean[~df_whoswho_ornitela_clean['date_death'].isna()])Great, we see that these two datasets are taking shape to serve as targets for the Ornitela time series dataset! Because the have the same schema (structure) we can just join them and get rid of any duplicates (e.g., taranaki or xena):df_mortality_target = pd.concat([df_mortality_ornitela_clean, df_whoswho_ornitela_clean], ignore_index=True)df_mortality_target.head()print(f\"There are {count_dup} deceased Ornitela vultures that are both in the who's who and mortality datasets.\")There are 26 deceased Ornitela vultures that are both in the who's who and mortality datasets.If we look at the duplicates, two interesting patterns appear:  There are some vultures that have been marked as deceased in the mortality dataset but not in the who’s who dataset  Some duplicates records have different dates of death - normally one or two days apartIn any case, this insight must indicates that the mortality dataset is a more recent source of truth, so for every duplicate we encounter we’ll keep the mortality record (the first):df_mortality_target = df_mortality_target.drop_duplicates(subset=['Movebank_id'], keep='first')df_mortality_targetSo finally we have our target dataset (with tags about mortality) and we can proceed to merge it with the Ornitela time series dataset to create an ML-trainable dataset.Constructing an ML training-ready Ornitela datasetBefore we join the target dataset with the Ornitela dataset, in the Exploratory Data Analysis (EDA) of the Ornitela we decided to only use a subset of columns that the most sense from an inference point of view:  event-id  individual-local-identifier  timestamp  location-long  location-lat  height-above-msl  ground-speed  acceleration-raw-x  acceleration-raw-y  acceleration-raw-z  external-temperatureLet’s filter our dataset with those columns and make sure that the timestamp is a timestamp datatype and not string:cols_ornitela_clean = [    'event-id',                                   # primary key    'individual-local-identifier',                # foreign key    'timestamp',     'location-long',     'location-lat',    'acceleration-raw-x',     'acceleration-raw-y',     'acceleration-raw-z',    'external-temperature',     'ground-speed',     'height-above-msl',]df_ornitela = df_ornitela[cols_ornitela_clean]df_ornitela['timestamp'] = pd.to_datetime(df_ornitela['timestamp'],                                          format='%Y-%m-%d %H:%M:%S', errors='coerce')df_ornitelaThis is a pretty large dataset with over a quarter of a million rows! Before we go ahead with the join, let’s see how many unique vultures this has in common with the mortality dataset we prepared above:ids_target = df_mortality_target['Movebank_id'].unique()ids_ornitela = df_ornitela['individual-local-identifier'].unique()ids_intersect = np.intersect1d(ids_target, ids_ornitela)print(f'The Ornitela and target datasets have {len(ids_target)} and {len(ids_ornitela)} unique vultures respecitvely.')print(f'Out of this number, they have {len(ids_intersect)} vultures in common:')print(\"-\"*80)print(ids_intersect)The Ornitela and target datasets have 102 and 110 unique vultures respecitvely.Out of this number, they have 99 vultures in common:--------------------------------------------------------------------------------['A00W' 'A01W' 'A03W' 'A05W' 'A09W' 'A10W' 'A13W' 'A15W' 'A16W' 'A18W' 'A19W' 'A20W' 'A22W' 'A29W' 'A31W' 'A32W' 'A33W' 'A35W' 'A36W' 'A38W' 'A39W' 'A52W' 'A53W' 'A55W' 'A56W' 'A57W' 'A58W' 'A73W' 'A75W' 'A76W' 'A78W' 'E00W' 'E01W' 'E03' 'E03W' 'E04W' 'E05W' 'E07W' 'E09W' 'E10W' 'E11W' 'E12W' 'E13W' 'E14W' 'E15W' 'E16W' 'E17W' 'E19W' 'E32W' 'E33W' 'E34W' 'E37W' 'E38W' 'E39W' 'E41W' 'E45W' 'J00W' 'J05W' 'J06W' 'J11W' 'J12W' 'J15W' 'J16W' 'J17W' 'J18W' 'J19W' 'J28W' 'J31W' 'J32W' 'J33W' 'J34W' 'J35W' 'J36W' 'J39W' 'J53W' 'J66W' 'T13B' 'T14W' 'T15W' 'T17W' 'T19B' 'T25B' 'T50B' 'T53B' 'T56B' 'T59W' 'T66W' 'T69B' 'T71W' 'T76W' 'T77W' 'T79W' 'T85W' 'T86B' 'T90B' 'T99B' 'Y26' 'Y26B' 'Y27B']Great, almost 100 vultures in common! Now, we can do an inner join with the relevant mortality records:df_ornitela_joined = (    df_ornitela    .merge(        df_mortality_target[['Movebank_id', 'date_death']],        how=\"inner\",        left_on=\"individual-local-identifier\",        right_on=\"Movebank_id\"    ))print(f\"The joined dataset has {len(df_ornitela) - len(df_ornitela_joined)} less records than the Ornitela dataset.\")print(f\"Out of these records, there are {len(df_ornitela_joined) - len(df_ornitela_joined.drop_duplicates(subset=cols_ornitela_clean))} duplicates.\")print('-'*80)df_ornitela_joinedThe joined dataset has 122020 less records than the Ornitela dataset.Out of these records, there are 0 duplicates.--------------------------------------------------------------------------------So we can see that we missed roughly 5% of the records in the join, which is not significant percentage.Next, we need to apply logic to create the mortality labels for each time series record. That is, for each record we compare if the timestamp of the event with the date death for that vulture (if it exists) and if timestamp &gt;= date_death, then we set a flag for that record as is_at_risk = \"Y\". The challenge we face is that for mortality we only have a date, whereas the events have a timestamp. We could subsample the timeseries data to only have one record per day, but for that we need to first check the sampling rate for each record to find out if the time separation between events is constant:print(f\"There are {len(df_ornitela['timestamp'].diff().value_counts())} different sampling rates:\")print('-'*60)df_ornitela['timestamp'].diff().value_counts()There are 22549 different sampling rates:------------------------------------------------------------0 days 00:10:00    5069550 days 00:10:01    1494270 days 00:09:59    1307730 days 00:10:02     796730 days 00:09:58     66405                    ...  0 days 12:33:59         10 days 08:32:11         10 days 12:13:52         10 days 13:10:43         10 days 03:30:13         1Name: timestamp, Length: 22549, dtype: int64pd.crosstab(    index=df_mortality_target[df_mortality_target[\"Movebank_id\"].isin(ids_intersect)][\"reason\"],    columns='% observations',     normalize='columns')*100            col_0      % observations              reason                        collision powerline      3.703704              electrocution      3.703704              poisoning      55.555556              unknown      37.037037      We can see that the Ornitela timeseries has a very uneven sampling rate. Thus, it’s not straight forward to subsample the dataset and be certain that we have one record per day per vulture.Moreover, poisoning corresponds to over half of the vulture deaths in the Ornitela time series data. This implies that the risk of death might not be instant and vultures that poisoned vultures that die on a given day might have been at risk of death for at least the last 24 hours or more.Thus, for a first iteration of the ML algorithm, I’ll make the following assumption to implement the mortality labelling logic:ASSUMPTION: For any vulture that has a non null date_death value, any timestamp within that date will be flagged as is_at_risk = \"Y\":# create a labelling column for the time series datasetrisk_evaluator = lambda row: \"N\" if row.timestamp &lt; row.date_death else (\"N\" if pd.isnull(row.date_death) else \"Y\")df_ornitela_joined['is_at_risk'] = (    df_ornitela_joined    .apply(risk_evaluator, axis=1))df_ornitela_target = df_ornitela_joined.copy()df_ornitela_target                  event-id      individual-local-identifier      timestamp      location-long      location-lat      acceleration-raw-x      acceleration-raw-y      acceleration-raw-z      external-temperature      ground-speed      height-above-msl      Movebank_id      date_death      is_at_risk                  0      16422103004      T59W      2020-08-28 04:27:58      35.013573      32.753487      -65.0      10.0      -1058.0      28.0      0.277778      368.0      T59W      2021-09-11      N              1      16422103005      T59W      2020-08-28 04:30:33      35.013290      32.753368      -33.0      -638.0      815.0      28.0      0.277778      368.0      T59W      2021-09-11      N              2      16422103006      T59W      2020-08-28 04:35:28      35.013302      32.753448      -17.0      -635.0      824.0      29.0      0.000000      368.0      T59W      2021-09-11      N              3      16422103007      T59W      2020-08-28 04:40:28      35.013493      32.753475      108.0      4.0      1044.0      31.0      0.000000      368.0      T59W      2021-09-11      N              4      16422103008      T59W      2020-08-28 04:45:37      35.013519      32.753521      60.0      -432.0      -1147.0      31.0      0.277778      368.0      T59W      2021-09-11      N              ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...              2251981      23854049823      E16W      2022-09-29 07:20:37      34.823700      30.868530      21.0      134.0      873.0      35.0      15.277778      911.0      E16W      NaT      N              2251982      23854049824      E16W      2022-09-29 07:30:38      34.841583      30.892664      37.0      227.0      1058.0      35.0      13.055556      802.0      E16W      NaT      N              2251983      23854049825      E16W      2022-09-29 07:40:38      34.831387      30.891184      47.0      315.0      1204.0      35.0      4.444444      934.0      E16W      NaT      N              2251984      23854049826      E16W      2022-09-29 07:50:38      34.851757      30.920616      36.0      238.0      1106.0      35.0      9.166667      731.0      E16W      NaT      N              2251985      23854049827      E16W      2022-09-29 08:00:38      34.895168      30.957092      32.0      273.0      1169.0      36.0      3.611111      750.0      E16W      NaT      N      2251986 rows × 14 columnsdf_ornitela_target[(df_ornitela_joined[\"is_at_risk\"] == \"Y\")]Let’s also print the the distribution of at risk vultures versus not at risk:pd.crosstab(index=df_ornitela_target[\"is_at_risk\"], columns='% observations', normalize='columns')*100            col_0      % observations              is_at_risk                        N      99.925133              Y      0.074867      The vast majority of time series records (over 99.9%) correspond to non-at-risk events. This is mainly due to two causes:  Death events are quite rare, especially over the timespan of a vulture’s deployment in the field  Death creates a statistical bias in the time series data: a vultures’ tag will be removed after their death, so we stopped seeing those records in the time series dataFor this project, the difference in the risk distribution might have dowstream consequences on what ML model we select, and whether we need to resample the data to make the difference less significant. For now, I will keep all the records to save the dataframe and later on we can resample this dataframe.Finally, let’s save the notebook so we can utilise it in the next step for training:save_target_df = Trueif save_target_df:    print(f\"Saving the Ornitela target dataframe:\")    print('='*100)    out_dir = './../data'    if not os.path.isdir(out_dir):        os.makedirs(out_dir)        print(f\"A {out_dir} directory was created\")        print('-'*60)    else:        pass    out_path = os.path.join(out_dir,'df_ornitela_target.parquet')    df_ornitela_target.to_parquet(out_path)    print(f\"The Ornitela target dataframe was succesfully saved in {out_path}.\")    print('='*100)Saving the Ornitela target dataframe:====================================================================================================The Ornitela target dataframe was succesfully saved in ./../data/df_ornitela_target.parquet.===================================================================================================="
  },
  
  {
    "title": "Nesher Bari - Exploratory Data Analysis",
    "url": "/posts/nesher_bari-part2/",
    "categories": "portfolio",
    "tags": "ai/ml, data-science",
    "date": "2023-08-23 11:00:00 +1000",
    





    "snippet": "In the previous part of this series, we introduced the Nesher Bari project, which aims to build an ML solution to accelerate vulture conservation (see more details in Wildlife.ai’s website).In this post, we dive into more technical details and explore one of the main datasets in the project: the Ornitela dataset.import librariesimport osimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsfrom jupyterthemes import jtplotCOLORS = sns.color_palette(\"deep\", 12).as_hex()pd.set_option('display.max_columns', 100)darkmode_on = Trueif darkmode_on:    jtplot.style(theme='grade3', ticks=True, grid=True)Load &amp; Extract Data: Quick Overviewdf_ornitela_raw = pd.read_csv('./../data/Ornitela_Vultures_Gyps_fulvus_TAU_UCLA_Israel_newer.csv')df_ornitela_raw.head()                  event-id      visible      timestamp      location-long      location-lat      acceleration-raw-x      acceleration-raw-y      acceleration-raw-z      bar:barometric-height      battery-charge-percent      battery-charging-current      external-temperature      gps:hdop      gps:satellite-count      gps-time-to-fix      ground-speed      heading      height-above-msl      import-marked-outlier      gls:light-level      mag:magnetic-field-raw-x      mag:magnetic-field-raw-y      mag:magnetic-field-raw-z      orn:transmission-protocol      tag-voltage      sensor-type      individual-taxon-canonical-name      tag-local-identifier      individual-local-identifier      study-name                  0      16422103004      True      2020-08-28 04:27:58.000      35.013573      32.753487      -65.0      10.0      -1058.0      0.0      92      0.0      28.0      1.7      5      159.69      0.277778      87.0      368.0      False      1046.0      -0.621      0.036      0.014      GPRS      4100.0      gps      Gyps fulvus      202382      T59w      Ornitela_Vultures_Gyps_fulvus_TAU_UCLA_Israel              1      16422103005      True      2020-08-28 04:30:33.000      35.013290      32.753368      -33.0      -638.0      815.0      0.0      92      15.0      28.0      1.7      5      16.04      0.277778      47.0      368.0      False      1386.0      -0.603      -0.330      -0.495      GPRS      4103.0      gps      Gyps fulvus      202382      T59w      Ornitela_Vultures_Gyps_fulvus_TAU_UCLA_Israel              2      16422103006      True      2020-08-28 04:35:28.000      35.013302      32.753448      -17.0      -635.0      824.0      0.0      93      15.0      29.0      1.8      5      11.44      0.000000      113.0      368.0      False      2047.0      -0.575      -0.367      -0.493      GPRS      4108.0      gps      Gyps fulvus      202382      T59w      Ornitela_Vultures_Gyps_fulvus_TAU_UCLA_Israel              3      16422103007      True      2020-08-28 04:40:28.000      35.013493      32.753475      108.0      4.0      1044.0      0.0      93      0.0      31.0      1.8      5      11.53      0.000000      52.0      368.0      False      1928.0      0.040      -0.045      -0.659      GPRS      4108.0      gps      Gyps fulvus      202382      T59w      Ornitela_Vultures_Gyps_fulvus_TAU_UCLA_Israel              4      16422103008      True      2020-08-28 04:45:37.000      35.013519      32.753521      60.0      -432.0      -1147.0      0.0      93      0.0      31.0      2.0      4      20.52      0.277778      290.0      368.0      False      496.0      -0.314      0.111      -0.113      GPRS      4106.0      gps      Gyps fulvus      202382      T59w      Ornitela_Vultures_Gyps_fulvus_TAU_UCLA_Israel      Let’s explore some basic information about the Ornitela dataset, mainly shape and schema:print(f\" n_rows: {df_ornitela_raw.shape[0]} \\n n_columns: {df_ornitela_raw.shape[-1]}\") n_rows: 2374007  n_columns: 30print(f\"dataset schema:\")print(\"=\"*60)df_ornitela_raw.info()dataset schema:============================================================&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 2374007 entries, 0 to 2374006Data columns (total 30 columns): #   Column                           Dtype  ---  ------                           -----   0   event-id                         int64   1   visible                          bool    2   timestamp                        object  3   location-long                    float64 4   location-lat                     float64 5   acceleration-raw-x               float64 6   acceleration-raw-y               float64 7   acceleration-raw-z               float64 8   bar:barometric-height            float64 9   battery-charge-percent           int64   10  battery-charging-current         float64 11  external-temperature             float64 12  gps:hdop                         float64 13  gps:satellite-count              int64   14  gps-time-to-fix                  float64 15  ground-speed                     float64 16  heading                          float64 17  height-above-msl                 float64 18  import-marked-outlier            bool    19  gls:light-level                  float64 20  mag:magnetic-field-raw-x         float64 21  mag:magnetic-field-raw-y         float64 22  mag:magnetic-field-raw-z         float64 23  orn:transmission-protocol        object  24  tag-voltage                      float64 25  sensor-type                      object  26  individual-taxon-canonical-name  object  27  tag-local-identifier             int64   28  individual-local-identifier      object  29  study-name                       object dtypes: bool(2), float64(18), int64(4), object(6)memory usage: 511.7+ MBLet’s explore if there are any duplicated event entries or any entries with null values:print(f\"duplicated rows in the Ornitela dataset:\")print(\"=\"*60)print(df_ornitela_raw.duplicated().sum())print(f\"duplicated rows in the Ornitela dataset:\")print(\"=\"*60)print(df_ornitela_raw.isna().sum())duplicated rows in the Ornitela dataset:============================================================0duplicated rows in the Ornitela dataset:============================================================event-id                           0visible                            0timestamp                          0location-long                      1location-lat                       1acceleration-raw-x                 0acceleration-raw-y                 0acceleration-raw-z                 0bar:barometric-height              0battery-charge-percent             0battery-charging-current           0external-temperature               0gps:hdop                           0gps:satellite-count                0gps-time-to-fix                    0ground-speed                       0heading                            0height-above-msl                   0import-marked-outlier              0gls:light-level                    0mag:magnetic-field-raw-x           0mag:magnetic-field-raw-y           0mag:magnetic-field-raw-z           0orn:transmission-protocol          0tag-voltage                        0sensor-type                        0individual-taxon-canonical-name    0tag-local-identifier               0individual-local-identifier        0study-name                         0dtype: int64We can see that the Ornitela dataset contains no duplicates but 1 entry with null latitude and longitude.Let’s make sure we drop any null values and even duplicates (even if there are non) and then extract some basic stats from each of the attributes:df_ornitela = (    df_ornitela_raw    .copy()    .drop_duplicates()    .dropna())df_ornitela.describe().apply(lambda s: s.apply('{0:.5f}'.format))                  event-id      location-long      location-lat      acceleration-raw-x      acceleration-raw-y      acceleration-raw-z      bar:barometric-height      battery-charge-percent      battery-charging-current      external-temperature      gps:hdop      gps:satellite-count      gps-time-to-fix      ground-speed      heading      height-above-msl      gls:light-level      mag:magnetic-field-raw-x      mag:magnetic-field-raw-y      mag:magnetic-field-raw-z      tag-voltage      tag-local-identifier                  count      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000      2374006.00000              mean      20552869208.05338      35.41146      29.08398      25.38844      519.36946      800.00706      0.00000      92.90503      7.60352      34.75356      1.51000      7.60744      29.60435      4.39986      178.27466      705.50741      688.54746      0.12841      -0.10202      -0.24087      4121.36826      206449.77052              std      2181978669.49922      2.80387      4.93453      101.53411      301.16978      259.57626      0.00000      13.07223      12.04568      4.85817      1.06649      2.30207      25.71204      6.77396      105.65555      529.72801      841.40669      1.18763      0.78688      0.86235      84.77994      5381.99189              min      16105780011.00000      0.00000      2.00003      -1606.00000      -1329.00000      -1315.00000      0.00000      0.00000      0.00000      0.00000      0.00000      3.00000      0.00000      0.00000      0.00000      -1998.00000      0.00000      -6.71800      -13.78400      -6.31800      0.00000      202359.00000              25%      18884479400.25000      34.82033      30.75541      -17.00000      234.00000      604.00000      0.00000      91.00000      0.00000      33.00000      1.00000      6.00000      12.76000      0.00000      85.00000      431.00000      17.00000      -0.06700      -0.37700      -0.56200      4095.00000      202377.00000              50%      20991679678.50000      35.00437      30.83160      27.00000      568.00000      817.00000      0.00000      100.00000      0.00000      35.00000      1.30000      7.00000      16.16000      0.27778      179.00000      515.00000      131.00000      0.25500      -0.00500      -0.17100      4155.00000      202398.00000              75%      22502683086.75000      35.22010      30.95092      73.00000      799.00000      992.00000      0.00000      100.00000      15.00000      38.00000      1.70000      9.00000      35.86000      9.44444      270.00000      884.00000      1676.00000      0.55200      0.34500      0.12800      4178.00000      213563.00000              max      23854497939.00000      45.30416      40.02653      1798.00000      1810.00000      2040.00000      0.00000      100.00000      57.00000      68.00000      15.90000      22.00000      272.37000      783.33333      360.00000      9992.00000      2047.00000      28.17200      4.78000      14.32900      4203.00000      213596.00000      Also, let’s take a quick look at the frequency of values for some key categorical attributes:cols_categorical = [    'orn:transmission-protocol',     'individual-taxon-canonical-name',     'individual-local-identifier',     'study-name']# print frequency tables for each categorical featurefor column in cols_categorical:    display(pd.crosstab(        index=df_ornitela[column],         columns='% observations',        normalize='columns'    )*100           )            col_0      % observations              orn:transmission-protocol                        GPRS      99.58252              SMS      0.41748                  col_0      % observations              individual-taxon-canonical-name                        Gyps      1.144563              Gyps fulvus      98.855437                  col_0      % observations              individual-local-identifier                        A00w      1.603534              A01w      0.366259              A02w      0.079781              A03w      1.865749              A04w      0.320934              ...      ...              T91b      0.832938              T99b      1.806609              Y26      0.006108              Y26b      1.052061              Y27b      1.870551      110 rows × 1 columns            col_0      % observations              study-name                        Ornitela_Vultures_Gyps_fulvus_TAU_UCLA_Israel      100.0      From this we notice a few remarks:      Most recorded events come from GPS sensors instead of SMS.        Most vultures are Griffon vultures (~99.9%), and the rest are only tagged as ‘Gyps’ (vultures): because of the small fraction of the latter, we will safely assume that ‘Gyps’ also refer to Griffon vultures.        All the data entries correspond to the ‘Ornitela Vultures Gyps fulvus’ project (as expected), in collaboration between UCLA and TAU: this means that we don’t have to filter the dataframe for that study.  It’s more difficult to observe the frequency of records for each identifier (corresponding to each unique vulture) in a tabular form. To appreciate that, let’s plot the percentage of observations for each vulture (based on tag-local identifier):vultures_rate = pd.crosstab(    index=df_ornitela['individual-local-identifier'],     columns='% observations',     normalize='columns')*100plt.figure(figsize=(32,8))plt.xticks(rotation = 70)                           # rotates X-Axis Ticks by 70-degreesplt.ylim([0,6])sns.set(font_scale=1.5)sns.barplot(    data=vultures_rate,     x=vultures_rate.index,     y=vultures_rate['% observations'])plt.show()In the figure above, we can see that the frequency of observations (sampling size) across different vultures is pretty even. Although there are differences of roughly ~1% across difference vultures, the sampling is not dominated by one or a group of vultures.Data DistributionIn this section, we look more in detail at the distribution of different attributes. Studying these attributes might help us understand if there are any systematic trends or biases in the data. It also helps us better understand the attributes from the dataset, thus facilitating feature selection too.First, we define a reusable function to plot a distribution for each attribute:# define an auxiliary function to draw several plots in a tight layoutdef plot_distributon(    cols,     stat='count',     bins=100,    log_transformation=False,):    plt.figure(figsize=(25, 7))    sns.set(font_scale=2)    for i, feature in enumerate(cols):        ax = plt.subplot(1, len(cols), i+1)        if log_transformation:            sns.histplot(                data=df_ornitela[cols],                 x=feature,                 stat=stat,                 bins=bins,                log_scale=(False,True)            )        else:            sns.histplot(                data=df_ornitela[cols],                 x=feature,                 stat=stat,                 bins=bins,            )LocationWe explore the probability distribution of latitude and longitude. This is important because the INPA only operates in Israel, therefore we wouldn’t like the dataset to contain many griffon vultures that flew away from the area.loc_cols = ['location-long', 'location-lat']df_ornitela[loc_cols].describe().apply(lambda s: s.apply('{0:.5f}'.format))ISRAEL_LAT_RANGE = (29.55805, 33.20733)ISRAEL_LONG_RANGE = (34.57149, 35.57212)# plot lat and long  distributionfig, axs  = plt.subplots(1,2, figsize=(25, 7))sns.set(font_scale=2)for i, col in enumerate(loc_cols):    sns.histplot(        data=df_ornitela,        x=col,         bins=25,        stat='probability',        ax=axs[i]    )    axs[0].axvline(ISRAEL_LONG_RANGE[i], color=COLORS[1], linewidth=2)    axs[1].axvline(ISRAEL_LAT_RANGE[i], color=COLORS[1], linewidth=2)plt.tight_layout()plt.show()Although some griffon vultures flew out of Israel, we can see that the vast majority of records occurred in Israel.In the future, we could have a more fine-grained analysis to potentially improve the performance of an ML model. This would mainly involve getting rid of events that didn’t occur in the Negev desert in Israel, where INPA focuses their conversation efforts.HeightHeight might be a potential telling feature as outliers might correspond to a death related event, although change in height for a particular vulture would be more telling (this might be encoded in acceleration). Let’s plot the probability distribution of height:plt.figure(figsize=(14,7))sns.histplot(    data=df_ornitela,     x=df_ornitela['height-above-msl'],     stat='probability',    bins=80)plt.axvline(    df_ornitela['height-above-msl'].median(),     color=COLORS[1],    label=f\"median = {df_ornitela['height-above-msl'].median()} m\")plt.legend(loc=0, prop={'size': 20})plt.xlim(-500,3000)plt.show()We get a tail-end distribution with a median altitude of roughly 515m. This reflects the known fact that griffon vultures tend to stick around higher altitudes.We will also print the percentile distribution for reference:Ground SpeedGround speeds will tend to be very specific decimal numbers. Therefore, the challenge with looking at its distritbution is that it’ll be very sparse. To make up for this sparness, we can bin the data and plot the probability distribution:plt.figure(figsize=(14,7))sns.histplot(    data=df_ornitela,    x=df_ornitela['ground-speed'],     stat='probability',    bins=75,)plt.xlim([0, 200])plt.show()We see that it’s very much a tail-end distribution with the tail just around 25m/s. Therefore, most likely the speed of a vulture will be below 20 m/s. However, there’s some interesting behaviour at play. We binned the data in 75 bins but we can only see 3 in the plot above. I did bound the x-axis between speeds of 0 to 200, but still this doesn’t really explain all the missing bins. Let’e see if a log-plot of the count distribution without x-axis boundaries can give us any insights:plt.figure(figsize=(14,7))sns.histplot(    data=df_ornitela[df_ornitela['ground-speed']&gt;40],     x=df_ornitela['ground-speed'],     stat='count',    bins=75,    log_scale=(False,True),)plt.show()df_ornitela[['ground-speed']].describe().apply(lambda s: s.apply('{0:.5f}'.format))                  ground-speed                  count      2374006.00000              mean      4.39986              std      6.77396              min      0.00000              25%      0.00000              50%      0.27778              75%      9.44444              max      783.33333      Now we can see a few outliers with ground-speed values of slightly more than 400 [m/s] and slightly less than 800 [m/s]. This seems excesively large ground speeds for Griffon vultures, and it still so it must a malfunctioning of the tracking device.The summary statistics also doesn’t seem to indicate why we see these behaviour with missing bins. Let’s try to filter the dataset for speed values less than 80 m/s and replot the probability distribution of speed:df_ornitela_filtered = df_ornitela[df_ornitela['ground-speed'] &lt; 80]print(    f\"Number of records where vultures are moving at speeds greater than 80m/s:\",    f\"{1 - (len(df_ornitela_filtered)/len(df_ornitela)):.2e}\")print(\"=\"*100)plt.figure(figsize=(14,7))sns.histplot(    data=df_ornitela_filtered,     x=df_ornitela_filtered['ground-speed'],     stat='probability',    bins=100,)plt.xlim([0, 25])plt.show()Number of records where vultures are moving at speeds greater than 80m/s: 1.68e-06====================================================================================================This distribution is a much more telling picture of speed. It tells us that vultures are most likely to be static or moving at speeds below 2 m/s, which probably means they’re moving on the ground. Moving forward, we probably want to implement this filter in our dataset unless those outliers really represent dying vultures according to experts. In any case, these only represent 1.5 million of a fraction of the Ornitela sample.AccelerationNow we take a look at a very interesting attribute: acceleration which is measured in the 3 dimensional x-, y- and z-xis. First, let’s have an overall look at the desntiy distribution for acceleration in each direction:cols_acceleration = [    'acceleration-raw-x',     'acceleration-raw-y',    'acceleration-raw-z']plot_distributon(cols_acceleration, stat='density', bins=120)plt.tight_layout()plt.show()These a very interesting and diverse distributions! Going through each one (left to right above):      x-axis: It appears that the acceleration in this axis is mainly Gaussianly distributed around 0. Compared to the other directions, it’s surprising how symmatricaly distributed the acceleration is.        y-axis: This deviates more from a Gaussian shape, though one could argue it resembles a multi-modal Gaussian distribution. The surprising remark of this distribution is that the acceleration values are overall positive as opposed to the acceleration in the x-axis.        z-axis: Finally, the acceleration in the z-axis has a Gaussian-like shape without a defined peak. Again, we see that there are no negative acceleration values in this direction (except for a tiny bump around acceleration-raw-z= -500 ).  Moving to interpreting these results, first one should note that griffon vulture are gliders, meaning that they minimise flapping and aim to optimise air currents. This might explain why the x-axis acceleration is distributed around 0. That is, if the wind is moves at constant speeds (not direction) and the amount of flapping is minimal, the values around 0 will correspond to fluctuations in the wind speed that are not significant. This gliding might also explain why most acceleration values in the y- and z- directions are negative. Mainly, it is unlikely that the wind will decelerate vultures sideways in a significant manner and as glidders, they make good use of convective air currents to move upwards.To see if we can unpack more insights, let’s plot the three density distributions in a log-scale:# plot with log transformation on the y-axisplot_distributon(    cols_acceleration,    stat='density',    bins=120,    log_transformation=True)plt.tight_layout()Apart from really highlighting the bump at 0 for the z-axis accleration, I don’t think this log-scale distributions are very telling. We will also print the summary statistics for reference:df_ornitela[cols_acceleration].describe().apply(lambda s: s.apply('{0:.5f}'.format))                  acceleration-raw-x      acceleration-raw-y      acceleration-raw-z                  count      2374006.00000      2374006.00000      2374006.00000              mean      25.38844      519.36946      800.00706              std      101.53411      301.16978      259.57626              min      -1606.00000      -1329.00000      -1315.00000              25%      -17.00000      234.00000      604.00000              50%      27.00000      568.00000      817.00000              75%      73.00000      799.00000      992.00000              max      1798.00000      1810.00000      2040.00000      PressureLet’s have a look at the distribution of pressure as measured by barometric height (see more details here)plt.figure(figsize=(14,7))sns.histplot(    data=df_ornitela,     x=df_ornitela['bar:barometric-height'],     bins=80,)plt.show()We see that the pressure is not very informative because it’s always 0 so we will ignore this attribute from the analysis.Satellite CountThis counts how many satellites is used to produced each record. Plotting the count distribution:plt.figure(figsize=(14,7))sns.histplot(    data=df_ornitela,     x=df_ornitela['gps:satellite-count'],     bins=15,)plt.axvline(    df_ornitela['gps:satellite-count'].median(),     color=COLORS[1],     label=f\"median = {df_ornitela['gps:satellite-count'].median()}\",)plt.legend(loc=0, prop={'size': 20})plt.show()We can see that most frequently around 7 satellites are used to produce a data entry. However, this attribute might not be critical for the first version of an ML algorithm.TemperatureNow let’s look at another very interesting attritbute: temperature. Assuming this corresponds to the temperature of the vulture, this can be a very good proxy for a death event. Especially if we look at the history of low temperature events associated with death, we might be able to distinguish between different types of death: lead poisoning, collision or hunting for example. Plotting the distribution of temperature:plt.figure(figsize=(14,7))sns.histplot(    data=df_ornitela,     x=df_ornitela['external-temperature'],     stat='percent',    bins=50,)plt.axvline(    df_ornitela['external-temperature'].median(),     color=COLORS[1],     label=f\"median = {df_ornitela['external-temperature'].median()} ˚C\")plt.legend(loc=0, prop={'size': 12})plt.show()Interestingly, there are quite a few low temperature values and a bump around 0 (around 1%). Given that death is a rare event, the latter is an outlier that may be hidden by the abundance of values around 35˙C. Let’s see if the logged-scale count distribution can resolve that:# Logarithmic transformation on the y-axisplt.figure(figsize=(14,7))sns.histplot(    data=df_ornitela,     x=df_ornitela['external-temperature'],    stat='percent',    bins=50,     log_scale=(False,True),)When looking at at the plot above, now we really see the highlighted bump around 0 as a very interesting outlier, corresponding to more than 1,000 records of dead vultures.Correlation OverviewScatter Matrix PlotFinally, a scatter matrix plot allows us to see if there are any correlations between numerical features, which will feed into feature selection. If there’s a strong correlation (or various) between two attributes, this will tell us that one of the features is redundant to feed into an ML model.Since plotting a scatter matrix is very computationally demanding, I’ll subsample the data set:# extract numerical cols of interestnum_cols = [    #'location-long',     #'location-lat',    'acceleration-raw-x',     'acceleration-raw-y',    'acceleration-raw-z',    'ground-speed',    'external-temperature',    'height-above-msl',]df_num = df_ornitela[num_cols].sample(len(df_ornitela)//10)g = sns.pairplot(df_num)for ax in g.axes.flatten():    ax.set_xlabel(ax.get_xlabel(), rotation = 90)                           # rotate x axis labels    ax.set_ylabel(ax.get_ylabel(), rotation = 0)                            # rotate y axis labels    ax.yaxis.get_label().set_horizontalalignment('right')                   # set y labels alignmentplt.tight_layout()There are no obvious correlations at first glance. We can note however that the deviation of the scatter is quite great and that we see quite a lot of clustering between the attributes. In any case, we don’t have to remove attributes due to correlations.ConclusionIn this post, we performed an Exploratory Data Analysis (EDA) on the Ornitela dataset, which enables with a better understanding on how the dataset is characterised. Moreover, it gave us an overview of what attributes of the dataset are most important for developing an ML algorithm that can predict a likelihood of high-risk of death based on the attributes (e.g., latitude, longitude, speed, etc).From a causal inference perspective, I think the most important attributes for the first version of an ML algorithm are the following:  event-id  individual-local-identifier  timestamp  location-long  location-lat  height-above-msl  ground-speed  acceleration-raw-x  acceleration-raw-y  acceleration-raw-z  external-temperatureOnce we have done a first feature selection, we can go ahead and prepare the dataset to be fed into an ML model. Stay tuned for the next steps!"
  },
  
  {
    "title": "AI & Morality - What is Algorithmic Fairness?",
    "url": "/posts/fml-part2/",
    "categories": "blog",
    "tags": "ai/ml, data-science, ethics, thought-leadership",
    "date": "2023-06-07 11:00:00 +1000",
    





    "snippet": "This blogpost was originally released through the Good Data Institute (GDI), where I work as a Fellow to to give not-for-profits access to data analytics support &amp; tools for social and environmental good. If you’d like to learn more about GDI, check their website here.Picture taken in Aotearoa New Zealand   If you are arrested in the U.S. today, COMPAS or an algorithm like it will likely influence if, when and how you walk free. ‘Ethics for Power Algorithms’ by Abe Gong (Medium)In the previous part of this series, I argued that the productionisation of AI/ML has and will continue to amplify unfairness and societal bias. In short, societal bias refers to the conglomeration of non-statistical social structures that have historically harmed underrepresented demographic groups. This type of bias makes fair decision-making by an algorithm more difficult or even impossible. For example, in the late 19th and early 20th centuries, the Jim Crow laws unfairly oppressed African-Americans in the Southern United States. In turn, these laws would have induced statistical bias since more data about African-American ‘offenders’ would have been collected.The cost of deploying AI/ML algorithms into production is ever decreasing due to technological advances in hardware, as well as the appearance of cloud services such as Amazon Web Services or Microsoft Azure (decreasing the cost of storage and compute). This, along with the revolution in generative AI that we are currently experiencing (e.g., ChatGPT), will enable many practitioners to get AI/ML algorithms out into the world in a ‘deploy first, ask questions later’ fashion. That is, not understanding the societal impacts and  as a consequence of this, we will see biased AI/ML products being deployed at an ever growing rate.To mitigate these risks, we must program a sense of fairness into these AI/ML algorithms. However, for data practitioners, that is no easy task because fairness is a sociological and ethical concept that often lies outside their comfort zone of technology and mathematical optimisation. For example, in 2017 Amazon scrapped a ‘secret’ AI/ML recruiting tool that favoured hiring males. Because the training data was composed of recruitment information from a male-dominant industry (IT), the algorithm ‘learned’ that males were more preferable for IT roles.In this post, I’ll provide a quick walkthrough on algorithmic fairness: outlining some existing definitions from this field and discussing why developing a golden standard for algorithmic fairness is so complicated. To illustrate this discussion, I will design a simplified version of a recidivism AI/ML algorithm. This type of algorithm is used to predict the likelihood that an ex-convict will re-offend. In turn, this prediction can be used to inform the passing of a sentence or setting the conditions of parole.If you’d like to learn how GDI has partnered with the non-for-profit Brother 2 Another to decrease recidivism using a different approach (from the one discussed in this article), read this blogpost here.Just Enough MLGetting from data to prediction involves many moving &amp; inter-twinned components. As a result, AI/ML products will be complex and dynamic constructs. Dealing with this complexity has led practitioners to develop patterns such as the ‘AI/ML Product Lifecycle’. In this section, I’ll give an example of an AI/ML algorithm and illustrate each component of its lifecycle.Imagine we would like to build a classifier that will label ex-convicts as low- or high-risk to be re-arrested within some years from release based on a set of attributes (e.g., how much they earn, in what district they live, whether they went to university or where they went to school). This classifier algorithm is a simplified version of the controversial COMPAS recidivism algorithm, which we discussed in the previous post. In short, this algorithm labels ex-convicts as high-risk or low-risk based on the algorithm’s estimated likelihood that these ex-convicts will reoffend within some time (e.g., two years) as we can see in the image below.Courtesy of Machine Bias by Propublica For example, following the ‘AI/ML Product Lifecycle’ diagram above, each step would look like the following with our recidivism example:      Data Collection &amp; Data Preparation: We curate a dataset representative of the problem we’re trying to solve. In the case of a recidivism algorithm like COMPAS, this might include gathering anonymised features about ex-convicts that both reoffended and didn’t reoffend within two years from release. Next, we’d have to clean up the data so it has no missing or erroneous values. This step typically requires first an Exploratory Data Analysis (EDA), where we investigate our dataset’s main characteristics and distribution.        Model Selection: Once we’ve gathered, cleaned and understood our data, we will have to choose an ML model into which we can feed our data. The problem we’re trying to solve will often constrain the models we should select. Since our example is classifying individuals based on some attributes, we will choose the logistic model (or logit model) as it’s the canonical model for this classification problem, given by the following formula (where Y=1 is a high-risk label, the x’s represent attributes from the datasets, and w’s act as weights to prioritise attributes):        Feature Engineering: This technique refers to designing ways of extracting features from the dataset. Feature engineering can take many forms, but for example, a transformation of the data (e.g., a z-transformation) might improve the performance of our model. Feature engineering can also be applied after model validation to improve our model’s performance. While improving a model’s performance can also be achieved for example by enhancing our training data or even choosing a different model altogether, feature engineering tends to be the best strategy we can choose.        Model Training &amp; Tuning: In this step, we split our data into training and test data. With the training data, the model learns the most optimal choice of weights (w’s in the formula above) that minimises the error. For example, attributes of the dataset such as criminal history or age might predict likelihood of recividism better than others. In this step, we can also tune the model’s hyperparameters to improve its performance, which mainly consists of tweaking the model parameters that are not optimised by the model (e.g., learning rate or the model’s initial parameters).    Model Evaluation: Once we have trained the model, we will feed it the test data to evaluate how it’s likely to perform with data it has never seen. Evaluation of the model will typically involve setting one or more acceptance criteria thresholds. If the model doesn’t meet those thresholds, we will go back and fine-tune the model to increase its performance. The evaluation process will depend on the problem we’re solving and the model we chose. In the case of a classification problem (like our example), we will use the confusion matrix (shown in the figure above) as the foundation for our model validation. In other words, for our logistic model, we need to consider four possible prediction outcomes when we feed each test data record. More generally, each of these belongs to either the positive or negative class, which in our example corresponds to low- or high-risk labels of recidivism respectively:      True Positive (TP): The model correctly labelled an ex-convict as high-risk who actually reoffended within two years from release.        True Negative (TN): The model correctly labelled an ex-convict as low-risk who had not reoffended within two years from release.        False Positive (FP): The model incorrectly labelled an ex-convict as high-risk when actually they hadn’t reoffended within two years from release.        False Negative (FN): The model incorrectly labelled an ex-convict as low-risk when in fact, they did reoffend within two years from release within two years from release.  Once we have these four scenarios, we can combine them to define validation metrics. For example, the most widely-used metric is accuracy, given by Acc = (TP+TN)/(TP+TN+FN+FP). We will come back to these metrics in more detail in the next section, as this step is the most important of the ‘AI/ML Product Lifecycle’ for algorithmic fairness.Note that this is a static and simplified version of a realistic scenario. To fully operationalise the model, we’d also need a deployment strategy, monitoring, continuous training and other CI/CD practices in place. Addressing these challenges (and more) has led to the emergence of MLOps, which is outside the scope of this post (you can read more about MLOps here).Defining Algorithmic FairnessDifferent biases can emerge from different phases of the AI/ML product lifecycle. I recommend reading this blog post to learn more about the ‘how’. Because AI/ML algorithms are susceptible to bias in most components of their AI/ML product lifecycle, the development of ‘fair’ AI/ML algorithms typically becomes a difficult task.‘Fair Machine Learning’ (Fair ML or FML for short) has emerged as the active field of research that aims to define fairness in ML algorithms technically. Generally, these definitions will require a performance metric to be the same across demographic groups.To illustrate some of these definitions, we will build on our recidivism algorithm above. In the last section, we learnt that before deploying our model, we would test its performance against some validation metrics.Although accuracy is frequently the only metric ML and data practitioners use, many have criticised it as an imprecise and vague metric (see this thread for more info). For example, it does not tell us how good the model is at misclassifying records into the positive or negative class. Luckily, more combinations of the confusion matrix will give us further metrics. Some examples include:      Precision (or Positive Predictive Value - PPV): Fairly popular metric that measures the positive predictive power of the model given by the following formula: PPV = TP/(TP+FP). For our recidivism example, this translates to measuring how well the algorithm is at correctly labelling ex-convicts as low-risk.        False Positive Rate (FPR): Fraction of misclassified positives (low-risk) out of all actual negative cases given by FPR = FP/(FP+TN). In other words, this would correspond to the fraction of records misclassified as low-risk out of all the actual high-risk individuals.        False Negative Rate (FNR): Fraction of misclassified negatives (high-risk) out of all actual positive cases: FNR = FN/(FN+TP). In our example, FNR would measure the fraction of misclassified high-risk individuals out of all actual low-risk cases.  Because most definitions of algorithmic fairness require one or more performance metrics to be the same across different groups, we will inevitably get many definitions of algorithmic fairness. As we discuss in the next section, all these definitions make developing a golden standard for algorithmic fairness difficult or even impossible.Fairness through UnawarenessFor the sake of simplicity and to follow other articles discussing COMPAS, imagine we have two demographic groups A and B, representing black and white ex-convicts respectively. One of the earliest definitions of algorithmic fairness is Fairness through Unawareness. This definition is satisfied when no sensitive attribute (race, gender, age, or disability) is used in the algorithm’s decision-making process. For our recidivism example, this would be satisfied if we removed all sensitive attributes from the training data (e.g., age or race) and our model satisfied the following formula: P(A, X_A) = P(B, X_B) if X_A = X_B. However, as we’ll learn in the next section, this definition has severe limitations.Statistical ParityMore generally, algorithmic definitions of fairness can be classified based on what outcomes they focus on: predicted or actual [1]. Definitions based on predicted outcome are the most naive and intuitive notions of fairness. An example belonging to this class is Statistical Parity. This definition requires the probability to be assigned to the positive class to be the same for both A and B, which would be given by (TP+FP)/(TP+FP+FN+TN). For our algorithm, this would be satisfied if the algorithm would be as good at labelling black ex-convicts as low-risk as labelling white ex-convicts as low-risk, regardless if the prediction was correct.  However, the limitation of this type of definition is that it only focuses on what the algorithm predicted rather than on whether it got the predictions right or wrong.Predictive EqualityThe limitation of statistical parity is covered by fairness definitions that consider actual outcome. An example is Predictive Equality, which is satisfied when both groups have the same False Negative Rate, i.e., FPR(A) = FPR(B). In the case of our COMPAS-like algorithm above, this would be satisfied if the fraction of high-risk individuals who were misclassified as low-risk is the same for both black and white ex-convicts.Equalised OddsEqualised Odds is an extension of Predictive Equality that also requires the False Negative Rate to be the same across both groups: FPR(A) = FPR(B) &amp; FNR(A) = FNR(B). In addition to FPR, in our example, this would be satisfied if the fraction of ex-convicts who were misclassified as high-risk when they were actually low-risk would be the same for both black and white individuals. This is a great definition of fairness that allows us to measure how poor a model is at misclassifying models, and what the disparities are across groups. However, as we’ll learn in the next section, Equalised Odds still has some limitations that are intrinsic to Fair ML.Challenges &amp; OpportunitiesMany challenges lie ahead to leverage algorithmic fairness, and Fair ML does not fall short of these challenges. For example, studies have shown that algorithms can satisfy some definitions of fairness while violating others [2]. However, this is not so much a constraint of the field of Fair ML but rather a challenge to the application of Fair ML into different real-world domains. In other words, in some applications e.g., the legal system, some definitions of algorithmic fairness will be more suitable than others, but this might be different for instance in the health care system.Moreover, most (if not all) of the examples of fairness definitions above have limitations. For instance, Fairness through Unawareness is a naive definition of fairness because of proxies, which are non-sensitive attributes that correlate with sensitive attributes. The most notable examples include annual income and education history, which most often (especially in the United States) will correlate to race and socio-economic status. Proxies like annual income or education history are apparent, so they can be spotted and removed from train and test datasets. Unfortunately, other more insidious proxies like postcode, online browsing and purchasing history make it extremely difficult (or even impossible) to remove all sensitive proxies from the algorithm’s decision-making process.In the case of Predictive Equality, we are making the implicit assumption that FPR is more fair than FNR. However, is that a reasonable assumption? From a fairness perspective, I’d argue that FNR captures fairness better than FPR because FNR measures the fraction of individuals who were misclassified as high-risk when in fact they were low-risk (and could potentially lead to them going to jail unfairly or getting an unfair parole). Nonetheless, from a civic safety point of view, one might argue that FPR is more important as you wouldn’t want to misclassify individuals as low-risk when they are very likely to reoffend.To remediate this, we could follow the pattern in Equalised Odds and keep requiring more performance metrics to be equal across groups to obtain a golden standard for algorithmic fairness. However, some of these performance metrics are incompatible with each other,  and thus, so will be some definitions of fairness too. For example, as pointed out by Chouldechova in her ‘The Frontiers of Fairness in Machine Learning’ paper:  except trivial settings, it is impossible to equalise FPR, FNR and PPV [(precision)] across protected groups simultaneously. [3]Even if we had a golden standard definition of algorithmic fairness, Fair ML has more fundamental limitations because of its statistical approach. That is, none of these definitions of fairness will guarantee fairness to anyone as an individual. Instead, Fair ML can only offer fairness to the ‘average’ member of the under-represented demographic group.Because of the landscape of fairness definitions that Fair ML offers, deciding what fairness definition is best suited will depend a lot on the problem domain. This presents an opportunity for data practitioners to collaborate and learn from domain experts. In our recidivism example, working with criminologists will be crucial to developing a fair AI/ML algorithm, as criminologists have conducted criminal justice assessments since the 1920s [4]. For example, evidence seems to suggest that males are more inclined towards violent crime than females (read this criminology article for more information).In summary, society cannot wholly rely only on technical definitions of algorithmic fairness, and ML Engineers cannot reinvent the wheel and establish what fairness represents across different domains.Closing ThoughtsThis post has introduced Fair ML, an active field of research that aims to define and measure fairness in algorithms technically. This field holds great promise as it can provide data and AI/ML practitioners with systematic tools for measuring unfairness aimed at mitigating algorithmic bias. These tools can be essential not only for data practitioners but for stakeholders and society as whole in a world of ever-expanding biased AI/ML algorithms for decision-making. Unfortunately, this field has some limitations, some of which we discussed in this post. Mainly, although there are at least 20 definitions of algorithmic fairness in the literature, most of these definitions have shortcomings. Moreover, studies have shown that some of these definitions are incompatible.Ideally, I’d personally like to see a golden standard with an agreed definition of algorithmic fairness. Then, AI/ML products would have to pass rigorous tests to be certified as algorithmically fair, similar to organic and cruelty-free products. However, I don’t see this happening anytime soon (if at all) because of the limitations of Fair ML as well as the overall complexity of this techno- and sociological topic. Realistically, organisations need to pick up one definition of fairness that aligns with their values and stick to it, making that explicit as well as making themselves accountable if they fail to follow this definition of fairness, as discussed in the previous post of this series.  in the end, it will fall to stakeholders – not criminologists, not statisticians and not computer scientists – to determine the tradeoffs […]. These are matters of values and law, and ultimately, the political process. [4]Algorithms cannot be the only resource for decision making. In a best-case scenario (when they are not biased), these algorithms can provide a prediction probability. ML Engineers cannot or should not be the only contributors that will champion this space. This, however, presents an opportunity for ML practitioners and stakeholders to embrace the interdisciplinary nature of Fair ML and work with problem-domain experts. In our example of the recidivism algorithm, it’d be foolish not to work with criminologists to build the algorithm as they have conducted criminal justice assessments since the 1920s [4].Finally, it is essential to acknowledge and respect the ‘hierarchy of risks’ when it comes to deployment and roll-out of AI/ML technology. For example, misclassifying ex-convicts as high-risk has more detrimental implications for their human welfare than misclassifying what movie they’re likely to enjoy. Likewise, misclassifying a person as not fit for a particular job role has more damaging implications to that person and their career than misclassifying what next product they’re most likely to buy online. AI/ML algorithms used for decision-making are in their very infancy, so whatever breakthrough or insight we might come across, we must put it under scrutiny and work iteratively.Academic References[1] Mitchell, S., Potash, E., Barocas, S., D’Amour, A., &amp; Lum, K. (2018). Prediction-Based Decisions and Fairness: A Catalogue of Choices, Assumptions, and Definitions. arXiv e-prints. https://arxiv.org/abs/1811.07867[2] Chouldechova, A. (2016). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. arXiv e-prints. https://arxiv.org/abs/1610.07524[3] Chouldechova, A., Roth, A. (2018). The Frontiers of Fairness in Machine Learning. arXiv e-prints. https://arxiv.org/abs/1810.08810[4] Berk, R., Heidari, H., Jabbari, S., Kearns, M., &amp; Roth, A. (2021). Fairness in Criminal Justice Risk Assessments: The State of the Art. Sociological Methods &amp; Research, 50(1), 3–44. https://doi.org/10.1177/0049124118782533"
  },
  
  {
    "title": "Nesher Bari Intro - Using Machine Learning for Vulture Conservation",
    "url": "/posts/nesher_bari-part1/",
    "categories": "portfolio",
    "tags": "ai/ml, data-science",
    "date": "2023-05-18 11:00:00 +1000",
    





    "snippet": "This blogpost was originally released through Widlife.ai, where I have contributed as a Data Ranger. If you’d like to learn more about Wildlife.ai’s mission of using AI/ML for nature conservation, check their website here.Griffon vulture (gyps fulvus) in flight at Gamla nature reserve. Photograph by Вых Пыхманн. Nature’s ​​SanitisersScavengers, and vultures in particular, provide an important ecosystem service, as they do the dirty work of cleaning up after animal death. This ensures that ecosystems stay healthy and safe from disease spread. Another well-known example of an ecosystem service is bee pollination. As in the case of bees, the potential of vulture extinction could pose a major risk to ecosystems worldwide.The main threat to vultures is the large-scale transformation of ecosystems caused by humans. This effect, known as anthropisation, drives the population decline of species and habitat loss, which threatens ecosystems worldwide.By collaborating with the Israel Nature and Parks Authority (INPA) through the Nesher Bari project (‘healthy vulture’ in Hebrew), we are leveraging an AI/ML solution to accelerate INPA’s conservation efforts towards Griffon vultures, the main endangered species of vultures in Israel.Why Vulture Conservation?Scavengers are essential to Earth’s ecosystem: they decrease the spread of disease by removing carcasses from the environment. There is a wide range of scavengers across the globe, both land- and ocean-bound: hyenas, coyotes, sharks, and killer whales are a few examples. However, vultures exclusively eat dead animal carcasses, and they have the most acidic stomach of all scavengers, allowing them to digest dead carcasses with botulinum toxins and rabies that would kill other scavengers. This makes them the most efficient and crucial family of scavengers on Earth. But as we will find out later, their acidic stomach also makes them the most susceptible to anthropisation.There are 23 known species of vultures on Earth.  In this project, we focus on one of the species that has the largest amount of data available: the Griffon vulture (Gyps fulvus, also known as the Eurasian Griffon). This amazing avian species, with a lifespan of up to 41.4 years and wingspan of up to 2.8m, can travel hundreds of kilometres in a day to find carcasses. Moreover, they can eat 10-20% of their body weight in dead animal carcasses (approx. 1.5kg) in one meal and then go without eating for a week (Harel et al 2016). Currently, the International Union for Conservation of Nature (IUCN) has listed ten vulture species as Critically Endangered, with the Griffon vulture being locally endangered in Israel. Without urgent conservation action, these species could become extinct in the area, and potentially lead to cascading ecological damage that could be detrimental to ecosystems worldwide.Vultures have seen a significant population decline in the last decades around the globe, which has been linked to anthropogenic causes. For example, in the 1990’s, India experienced a ‘Vulture Crisis’, where the three main species of vultures in the country lost 99% of their population. This event increased the population of ‘feral dogs’, which in turn caused an outbreak of rabies (a highly deadly disease), causing 50,000 human deaths in India and triggering a public health crisis that cost the government an estimated $34 billion.Threats to Griffon VulturesThe main human-related activities associated with vulture mortality and morbidity (disease) are:      Poisoning: The main form of poisoning occurs when vultures ingest lead (Pb) via the remains of shot game animals (Pain et al., 2019). Poisoning also occurs when vultures ingest either pesticides or poison baits used in agriculture.        Electrocution &amp; Collisions:  This type of incident has increased over the last decades and occurs when vultures touch wires or crash into power lines or wind turbines, the latter cause becoming more frequent in the last decade.        Vet Medicine: Some veterinary products used for livestock, such as the Non-Steroidal Anti Inflammatory Drug (NSAID) diclofenac, cause vulture mortality when they feed on livestock remains. This cause led to a health emergency mentioned above that cost the Indian government billions of dollars. Surprisingly, we still haven’t observed a ban for many of these veterinary drugs.        Furtive hunting: Although Griffon Vultures are a protected species across Europe, illegal killing is still one of the main causes of mortality for this species.  From all these causes of mortality and morbidity, studies have found that ingestion of lead through carcasses is the biggest contributor for human induced mortality amongst Griffon vultures, followed by collision (Ives et al., 2022). The acidic nature of their stomach (low pH) cannot break down the lead. This facilitates the absorption and transmission of lead into their bloodstream and tissue, which results in lead poisoning (Pain et al., 2019). Moreover, a study by Descalzo et al 2021 found that Griffon vultures are the most susceptible to lead poison amongst the 16 species of birds of prey in the study.Given the different threats to Griffon Vultures above, one of the easiest and most effective solutions would be to ban the use of lead ammunition for hunting and shooting. Leading the way, Denmark has become the first country in the world to completely ban lead hunting ammunition. Other countries are still lagging behind and refuse to take action against this potential human health risk. For example, in Spain lead ammunition is readily available, even though it has the highest population of Griffon vultures in Europe. Another key action is the regulation and law enforcement against the use of pesticides that are detrimental to Griffon vultures, implementing modifications to power lines (e.g., insulating wires) and wind farms.Vulture ConservationGiven the passive behavior of governments to mitigate vulture population decline through the legal system, the effort of researchers and conservation agencies (both Governmental and NGOs) is crucial to tackling the risk of extinction and the threat it would pose to ecosystems worldwide. Graphical representation of how veterinary drugs are threatening non-target endangered species such as vultures. Graphic by Plaza et al. 2022. In Israel, the population of Griffon vultures has been declining in the last years (read this article for more detailed info). Despite intensive conservation efforts by the Israel Nature and Parks Authority (INPA), some experts believe that this trend “suggest[s] we are approaching the extinction of this species in Israel and probably also through the Middle East”. For example, in the Negev desert in Israel, there are not enough carcasses to sustain the Griffon population (Spiegel et al. 2013). Thus sustaining this local population depends primarily on INPA’s effort and their 25 feeding stations over an area of 4,000 km2 (Harel et al. 2016b).The INPA also monitors a significant percentage of the total Griffon vulture population in Israel. With ultra-light (below 3% of the bird’s body weight) GPS transmitters, they are able to track key information about these creatures such as location, speed, altitude and body temperature. As part of this monitoring effort, vultures are searched and rescued if there are signs of some vultures being at risk of death based on the collected data. However, the current alert system is quite laborious and its accuracy is yet to be refined.Nesher Bari: Accelerating Vulture ConservationThe Nesher Bari’s team from Wildlife.ai is using animal tracking data to build an ML solution to accelerate vultures’ conservation effort. As we’ve learned, there are many potential causes of Griffon vulture mortality; therefore, it’s quite complex to predict what could cause the death of a vulture at any point in time. However, different inputs in the tracking devices might work as proxies to predict vulture mortality (e.g. location, altitude or body temperature). Building an ML algorithm that accelerates conservation efforts would be a great example of citizen AI/ML being used for environmental good.The Journey so farThe Nesher Bari team is formed by Sonia Moushaev and me as Data Scientists (aka Data Rangers), Ayan Mukhopadhyay and Victor Anton as Project Managers (aka Project Rangers), as well as the biologists Ofer Steinitz, Ohad Hatzofe and Kaija Gahm. Given that neither Sonia nor I have a background in ecology or conservation, the first hurdle has been wrapping our heads around the topic. By talking to experts, we’ve gained a bird’s-eye view of the technical and non-technical challenges before we start coding up the solution. The next hurdle is understanding the sources of data: how they are related and whether they can be combined to create more enriched datasets. This is a more Data Engineering endeavor that’s crucial to constrain what ML approaches we can take. For example, with a supervised learning approach, we could feed a model with examples of tracking data from vultures at their time of death. By detecting patterns in the data that go unnoticed by humans, the model could infer what inputs are best at predicting vulture mortality. However, this assumes that we have access to enough of these examples.At the time of writing, we have three main sources of data. Firstly, a time-series dataset provided by INPA, corresponding to 10 years of tracking data for 162 vultures. Secondly, a time series dataset provided by UCLA and Tel Aviv University, corresponding to 2 years of tracking data for 110 vultures. Finally, a look up table with more detailed information about individual vulture information (status, age, release date, … , etc) and most importantly, whether they are alive and the date of their death. As this is a living project, it’s likely that we will incorporate further data to enhance our AI/ML solution.Summary &amp; Next StepsWe’ve learned about the importance of scavengers to keep ecosystems healthy. In particular, vultures are the most efficient type of scavengers because of their acidic stomach, making their potential extinction a risk to ecosystems and global health. Despite their ecological importance, Griffon vultures are critically endangered in places like Israel due to anthropogenic (human) reasons. Unfortunately, governments worldwide are failing to intervene and mitigate the population decline of this species through the legal system e.g. not banning lead ammunition which is the major threat to Griffon vultures. This makes action by conservation agencies like the Israel Nature and Parks Authority (INPA) crucial to ensure vulture populations don’t go extinct.Using tracking time-series datasets of tagged Griffon vultures, we aim to build an ML model to quickly predict and identify when a Griffon vulture is at risk of death. This will equip INPA rangers with a data-driven tool to receive alerts more promptly and reduce the number of false alerts.In the upcoming months, we will be further exploring these datasets in order to better understand the technical challenges, inform our ML approach, and create a pipeline that can be fed to this ML model."
  },
  
  {
    "title": "The Productionisation of AI/ML - ‘The Golden Era’ or ‘The Wild West’?",
    "url": "/posts/fml-part1/",
    "categories": "blog",
    "tags": "ai/ml, data-science, ethics, thought-leadership",
    "date": "2022-10-17 11:00:00 +1000",
    





    "snippet": "This blogpost was originally released through the Good Data Institute (GDI), where I work as a Fellow to to give not-for-profits access to data analytics support &amp; tools for social and environmental good. If you’d like to learn more about GDI, check their website here.Picture taken in Aotearoa New Zealand Data Science has come to permeate almost every aspect of our society. Machine Learning and Artificial Intelligence (AI/ML) algorithms are being deployed at scale to facilitate our daily lives. Whether it’s filtering our spam emails, giving us directions, or nudging what we watch on streaming platforms, this technology has integrated seamlessly with our digital lives. These algorithms are also present in pivotal aspects of our lives, for example, by deciding whether we’ll be short-listed for an interview, or whether our bank loan will be approved. Meanwhile, in the public sector, algorithms are becoming more critical for decision-makers in both the policy and legal spheres.Over the last 50 years, Hollywood has painted a picture of a long-term AI apocalypse where civilisation would be destroyed by killer robots. However, the last decade has revealed a more sneaky reality: even today, AI/ML is creating harm in our society and it has the potential to further scale up injustice and unfairness.This post is the first part of a series about Ethics and Data Science. In this part, I will discuss the current picture mentioned above by describing the challenges present, outlining some examples of harmful AI/ML algorithms and discussing how this harm stems from the root. More importantly, I will propose some actionable items that organisations and data practitioners can take to leverage a fair use of data. Whether you are a data professional or simply interested in this topic, I hope to convince you of the importance of ethical practices in Data Science.ChallengesDespite Hollywood’s post-apocalyptic depiction of AI, I don’t think AI/ML algorithms are inherently evil. They are powerful tools that can be used for the benefit or detriment of our society. So how could they become harmful at all? One of the main issues we face is that these algorithms learn behaviour from data about how the world is, instead of how the world should be. To clarify this point, let’s consider the following quote [3]:  Statistical and machine learning models are designed to identify patterns in the data used to train them. As such, they will reproduce, to the extent that they are predictive, unfair patterns encoded in the data or unfairness in the problem formulation described above.That is, these algorithms have the power to amplify whatever is present in the data that the algorithm is trained on. If there are unfair or biased patterns in the data, these will be reflected in the output of the algorithm. The challenge is that by default, the data used as input in these algorithms is biased towards unfairness and underrepresentation. The concept of bias is useful as it can help us understand under what circumstances these AI/ML algorithms become unfair. But how does this harm originate in the first place and what exactly do I mean by bias?Bias in Data ScienceIn the context of this series, bias relates to outcomes of AI/ML algorithms that favour subsets of the population based on a human factor (age, gender, disability, ethnicity, ….). At the most general level, one can think of two main sources of bias related to Data Science:      Statistical Bias: The systematic discrepancy between the data used to train an AI/ML algorithm and the world as it is. This type of bias normally occurs when the training data is not representative of the full population [3]. For example, a study found that most off-the-shelf Computer Vision (CV) algorithms are trained with oversampled white facial images because the data was mostly collected in Western Countries [6].        Societal Bias: The ensemble of non-statistical social structures that make fair decision-making by a model more difficult or even impossible. For example, even if we could measure crime 100% accurately, there might be a normative bias due to an unjust policing system even if there’s no statistical bias.  Figure courtesy of Mitchell et al. 2018 [3]. Addressing issues of societal bias may require adjusting data collection processes, or unfortunately may not have a technical solution at all. Moreover, these two types of biases overlap with each other. For example, societal bias might affect the definition of how crime is defined, thus introducing statistical bias. In this series, I will mainly be focusing on statistical bias, as I think it’s easier to tackle algorithmically for individual data practitioners developing AI/ML products. However, because of the interconnectedness between societal and statistical bias, societal bias will naturally creep in throughout the series. For the interested reader, there have been some proposals in the literature to directly tackle societal bias in Data Science, for instance by embracing the notion of Intersectionalism (you can read more in this paper [2]).*Recognising these biases requires a retrospective understanding of how systematic injustice has manifested and developed in many domains over time. To leverage fair AI/ML products, professionals will benefit from upskilling in non-technical areas. For example, critical race theory will help practitioners understand the systematic underrepresentation of marginal groups in datasets. However, having to upskill in non-technical aspects of fairness will make many uncomfortable. I don’t mean morally uncomfortable (at least I would hope so), but in a constantly evolving tech-world where professionals have to constantly learn new tools and frameworks to keep up with industry, having to upskill in something outside of their domain of expertise will add more pressure to individuals as well as excuses for organisations to not develop AI/ML algorithms fairly. This is where organisations need to support individuals with the resources and encouragement to upskill in these non-technical areas. All organisations (whether big or small), should strive to make Data Ethics the norm and not a ‘would be nice’.Productionisation of AI/ML: ‘The Golden Era’ or the ‘Wild West’?In the last decade we’ve seen an explosion of harmful examples of AI/ML algorithms. However, these algorithms have been around for a lot longer in academia and industry research. For example, OXO was released in 1952 as the first algorithm that could play the perfect game of tic-tac-toe. More triumphantly, in 1997 IBM’s Deep Blue algorithm beat the world chess champion. So what has gone wrong in the last few years? In my opinion, the productionisation of AI/ML by market forces has accelerated the harm AI/ML algorithms pose to our society. But what exactly do I mean by productionisation of AI/ML?Both executives and investors are celebrating the ‘Golden Era of AI’, pouring millions into the development of AI/ML algorithms to derive profit while making their business success reliant on this technology. A survey of 1000 U.S. senior executives found that ‘93% of respondents say that emerging technologies, including deep learning, machine learning, and artificial intelligence, help their businesses be more competitive’.By now, there’s no doubt that AI/ML has immense potential to create business value. It’s no coincidence that the most successful tech companies are embedding AI/ML in the core of their products (think of Netflix, Spotify, Youtube, Amazon, …). However, when hearing business professionals talk of the ‘Golden Era of AI’ and its opportunities, I find it hard not to think of the Californian ‘Gold Rush’ in the 19th century. Historically, gold turned this land into the ‘Wild West’, and amongst other things, it led to the mass massacre and displacement of Native Americans in the area. While AI/ML has not got to that stage yet, the ‘AI/ML Rush’ is much more scalable and present in our society than gold mining. It’s also not short of harmful examples even today.For instance, in 2015 the Google Photos app mistakenly labeled African American people as ‘Gorillas’. More recently, OpenAI released DALL-E 2, an AI/ML algorithm that takes written text as input and generates an image from scratch. When instructed to produce images for trades that are stereotypically based on gender or race, DALL-E generates racial and gender-biased images. While OpenAI acknowledged this bias in their release statement, I don’t think acknowledgment leads to justification in this case. If you don’t think that’s enough examples, you can find more real-life examples of biased AI/ML products in this article.By this point, you might argue that all these examples might have offended subsets of the population, but have not caused ‘real harm’ to anyone. However, considering the influence of AI/ML in the legal system might change your opinion. For example, a software company called Equivant developed COMPAS, an AI/ML algorithm that uses data from previous offenders to predict the risk of recidivism (re-offending). After it had been deployed in production, this algorithm has been shown to suffer from both gender and racial bias. That is, COMPAS is more likely to tag black people and males as high-risk than white people and females respectively.Just like in the Hollywood depiction of the ‘Wild West’, it’s become a common practice for AI/ML organisations and data professionals to ‘shoot first and ask questions later’. Personally, I would be dubious of any AI/ML advocate who hails the ‘Golden Era of AI’ without acknowledging all the potential harms associated with AI/ML products.OpportunitiesWe’ve learned that data practitioners and organisations face challenges to leverage AI/ML products ethically. Given the lay of the land I’ve outlined above, there are many opportunities for organisations to leverage a fair use of data. The good news is that even smaller organisations have the potential to become champions in this space and lead the way.How can organisations leverage ethical practices around Data Science?Over the last years, we’ve seen many tech organisations come up with codes of conduct or set of principles for ethical data science (e.g. Microsoft, Google, IBM or Intel). More recently, these codes of conduct are becoming more tokenistic as we’ve seen companies being involved in AI/ML scandals even when their code of conduct was being ‘followed’. Here’s a list of actionable steps that organisations can use to mitigate the potential harms of AI/ML products:      Make your organisation accountable: Having an ethical code of conduct is (or should be) a statement of intent for wanting to leverage data science ethically. However, at the end of the day, intent is not nearly as important as outcome and results [4] (p.100-110). Therefore, organisations need to make themselves explicitly accountable when they deploy biased AI/ML products. ‘Faking ethics’ should be regarded as detrimental as faking data or results [8], and hopefully the legal system will leverage this accountability too.        Understand the trade-offs: Developing AI/ML products comes at a cost. For example, what should organisations do when there’s no available data from a given population sub-group, thus leading to statistical bias? One can spend more resources collecting an even number of representative data samples or tackle this challenge with algorithmic tools (e.g. data augmentation in Computer Vision). However, these methods are time- and resource-consuming, and as such organisations must be willing to pay the price of fairness, remembering that ‘minimising the error fits majority groups’ [1].        Strive for diversity across your organisation: Whether they realise or not, developer teams will be more susceptible to societal bias if all members come from a specific demographic (regardless of team members’ character and political alignments). Ensuring that all teams in your organisation are diverse across demographic factors (e.g. ethnicity, gender and disabilities) is crucial to flag and mitigate bias througout the AI/ML product lifecycle. In line with the item above, this very often comes at a cost (especially in recruitment), so organisations ought to be willing to pay the cost if they are committed to AI/ML fairness.        Auditability is not everything: Over the last years, there’s been more talk about the audibility of AI/ML products. The goal of auditability is to clearly document when decisions are made and, if necessary, backtrack to an earlier dataset and address the issue at the root. While good documentation and data lineage can be useful in this regard, auditability alone cannot guarantee an ethical use of data.        Context is crucial: AI/ML products are not developed and deployed into echo chambers. They are complex constructs created by humans whose outcomes affect a very large part of society. Therefore, having an understanding of the context surrounding an AI/ML algorithm is crucial for Fair Machine Learning. This affects every stage of the AI/ML product lifecycle: from data acquisition to cleaning, to interpretation of findings, and dissemination of the results. To minimise the chance of disregarding context, here are some questions you and your team can ask during development and deployment of AI/ML products [3], [8]:                  What’s the training dataset? How was it curated?                    What are the potential sources of societal and statistical bias?                    What’s the subset of the population that will be affected by the algorithm? Did they agree to be involved in the algorithm’s output?                    How will the deployed algorithm lead to decision-making?                    How are the stakeholders involved in the product? Does that affect the algorithm’s architecture?                  Enable and encourage debates around fair use of data: These conversations should be a topic that it’s discussed widely in the organisation. It cannot be something that only a couple data scientists in the organisation think about. Organisations thus have the responsibility to create a culture of psychological safety, where people feel comfortable speaking their own mind. This means debating and challenging what fair use of data entails, even if that’s at odds with the organisation’s financial interest.  Conclusions &amp; Next StepsIn this article, I hope I have convinced you of the risk that near- and mid-term productionisation of AI/ML can pose to society. We learned some examples of current AI/ML products that misclassify humans based on gender, ethnicity and socioeconomic status. We also learned that Data Ethics is a complex challenge because it encompasses both social, moral and technological areas.One of the greatest challenges in this topic is that organisations and individuals will develop and deploy biased AI/ML, many times even without realising because of statistical and societal bias. Organisations need to understand that ‘fairness is not one-size-fits-all’ [7] and to mitigate the risk of harmful AI/ML, I proposed some actionable steps that organisations and data practitioners can take. For example, organisations need to enable and encourage debates around the fair use of data, as well as make themselves accountable when they deploy harmful or unethical AI/ML algorithms. For data practitioners specifically, asking meaningful questions will help them understand the context around the human aspect of the data. Because of the many moving components of AI/ML algorithms (both in development and deployment), interpreting context ethically is one of the main challenges that data practitioners will have to face if they aspire to be moral professionals. Whether it is you or your organisation, by ignoring Data Ethics concerns you’re already making a choice, you’re just not aware of it.Another challenge for data practitioners is to understand how the potential roots of harm can appear as inputs in AI/ML algorithms, and from there architect these algorithms so that they produce fair outcomes instead of unfair ones. By collaborating with charities, at the Good Data Institute (GDI) we spend most of our time trying to showcase to the world how the power of Data Science can be used for social and environmental good.In the next part of this series, I’ll delve deeper into how we can algorithmically define a sense of fairness to hopefully program it into AI/ML algorithms. This will be a more technical endeavour that will lead me to introduce field Fair Machine Learning (FML). This turns out to be a very difficult task, but in a nutshell, FML aims to address statistical and societal bias by ensuring that the output of AI/ML algorithms doesn’t depend on sensitive inputs in a way that’s considered ‘unfair’. Examples of sensitive inputs include gender, ethnicity, socio-economic status, disability, or sexual orientation [5].  Statistical and structural biases can also be framed at different levels of perspective. For the interested reader, I recommend reading this article for a discussion of how different biases arise throughout the AI/ML product lifecycle.Academic References[1] Chouldechova, A. (2016). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. arXiv e-prints. https://arxiv.org/abs/1610.07524[2] Davis, J. L., Williams, A., &amp; Yang, M. W. (2021). Algorithmic reparation. Big Data &amp; Society, 8(2). https://doi.org/10.1177/20539517211044808[3] Mitchell, S., Potash, E., Barocas, S., D’Amour, A., &amp; Lum, K. (2018). Prediction-Based Decisions and Fairness: A Catalogue of Choices, Assumptions, and Definitions. arXiv e-prints. https://arxiv.org/abs/1811.07867[4] Noble, S. U. (2018). Algorithms of Oppression: How Search Engines Reinforce Racism. New York University Press.[5] Oneto, L., &amp; Chiappa, S. (2020). Fairness in Machine Learning. arXiv e-prints. https://arxiv.org/abs/2012.15816[6] Shankar, S., Halpern, Y., Breck, E., Atwood, J., Wilson, J., &amp; Sculley, D. (2018). No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World. arXiv e-prints. https://arxiv.org/abs/1811.07867[7] Suresh, H., &amp; Guttag, J. (2019). A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv e-prints}. https://arxiv.org/abs/1901.10002[8] Zook, M., Barocas, S., Boyd, D., Crawford, K., &amp; Keller, E. (2017). Ten simple rules for responsible big data research. PLOS Computational Biology, 13(3). https://doi.org/10.1371/journal.pcbi.1005399"
  },
  
  {
    "title": "NLP analysis on SMS text - part II",
    "url": "/posts/nlp-sms-part2/",
    "categories": "data-portfolio",
    "tags": "nlp, data-science",
    "date": "2022-08-04 20:34:00 +1000",
    





    "snippet": "In the first part of this Natural Language Processing (NLP) series, I pre-processed a SMS dataset using spaCy. In this post, I’ll use the Python library sklearn to extract some insights from the dataset.Because we have a text-based dataset, we need to turn words into numerical objects to carry out our analysis. This leads us to the concept of vectorisation.VectorisationIn NLP, vectorisation is the process of mapping words in the corpus to numerical features so that we can more easily analyse the dataset mathematically - either by means of a statistical analysis or by feeding the vector into a Machine Learning algorithm. In this post I won’t be using any Machine Learning Algorithms.Bag-of-Words (BOW)When it comes to vectorising the corpus (text-dataset), the most naive approach is to create a Bag-of-Words through the CountVectorizer() object. This technique counts the number of times a lemma appears in the corpus. Once we have the number of counts for each lemma, we can go ahead and build a wordcloud to visualise what lemmas are most common in these SMS messages. Wordclouds give more weight to words that appear more frequently by scaling them up in size.As mentioned above, we’ll use sklearn to carry out the vectorisation:import numpy as npfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizerfrom wordcloud import WordCloud# create CountVectorizer() object and generate bow matrixcount_vectorizer = CountVectorizer(    lowercase=True,    stop_words='english',     ngram_range=(1,1))bow_matrix = count_vectorizer.fit_transform(lemmas)feature_names_bow = count_vectorizer.get_feature_names_out()# get word count to create a wordcloudword_count = np.asarray(bow_matrix.sum(axis=0)).ravel().tolist()wcount_dict = {word: count for word, count in zip(feature_names_bow, word_count)}def black_color_func(word, font_size, position, orientation, random_state=None, **kwargs):            return(\"hsl(0,100%, 1%)\")def make_wordcloud(    x,     bg_color='white',    cloud_width=3500,     cloud_height=2000,     maxwords=500):        plt.figure(figsize=(16,9))    cloud = WordCloud(        font_path='./data/arial-unicode-ms.ttf',         background_color=bg_color,         width=cloud_width,         height=cloud_height,         max_words=maxwords)    cloud.generate_from_frequencies(x)    cloud.recolor(color_func = black_color_func)    plt.imshow(cloud, interpolation=\"bilinear\")    plt.axis(\"off\")    plt.show()# build wordcloudmake_wordcloud(wcount_dict)Interesting! Some of the words that come up most often across all the corpus are ‘thank’, ‘know’, ‘good’, ‘come’, ‘night’, … This is quite intuitive as these seem very common words one would use in short-format messaging (e.g. ‘thank you’, ‘I’m coming’, ‘I think so’, …)Term Frequency Inverse-Document Frequency (TF-IDF)A less naive approach to capture important words is the Term Frequency Inverse-Document Frequency (tf-idf). Unlike a classical bag-of-words approach, tf-idf takes into account the number of documents in the corpus that contain each words. This helps highlight more special words that are common only in very few documents, and weighs down very common words across all documents. For more information on how TF-IDF is calculated, see its Wikipedia article here.# create TfidfVectorizer() object and generate tfidf sparse matrixtfidf_vectorizer = TfidfVectorizer(    stop_words='english',    lowercase=True)tfidf_matrix = tfidf_vectorizer.fit_transform(lemmas)feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()# get word count to create a wordcloudweight_count = np.asarray(tfidf_matrix.sum(axis=0)).ravel().tolist()tfidf_count = np.array(weight_count)*np.array(word_count)if np.all(feature_names_tfidf == feature_names_bow):    tfidf_dict = {word: count for word, count in zip(feature_names_tfidf, tfidf_count)}# build tfidf wordcloudmake_wordcloud(tfidf_dict)Very similar to the BOW approach! We can see that some words like ‘hi’ become less important while other words like ‘yeah’ and ‘lol’ become more relevant, which is surprising as I’d expected these words to still be very frequent across all SMS.Part-of-Speech (POS) taggingMaking use of the spacy.nlp() pipeline which we created in part I, we can do more sophisticated analysis like Part-of-Speech tagging (POS). Loosely speaking, POS can be described as the process of capturing context in a corpus by labelling the words in the corpus according to the element of speech they belong to (noun, adjective, adverb, …).The pos tags were extracted and computed while pre-processing the data, but we can make a dataframe to make it clearer. Originally, I set out to build the df_pos in the following way:pos_dict = {word:tag for d_i in pos_tags for word, tag in d_i.items()}df_pos = pd.DataFrame.from_dict({    'word': pos_dict.keys(),    'pos_tag': pos_dict.values()})print(df_pos.sample(n=5, random_state=5))print(f'dataframe entries: {len(df_pos)}')         word pos_tag4654      fee    NOUN942    repair    NOUN2650  himself    PRON1104    spent    VERB4361     goot    NOUNdataframe entries: 7888However, this won’t work because dictionaries can’t have duplicate keys, so our dataframe will only have one entry per fixed word. Instead, we can use a list of tuples to create the POS dataframe:pos_data = [(word,tag) for d_i in pos_tags for word, tag in d_i.items()]df_pos = pd.DataFrame(pos_data, columns=['word', 'pos_tag'])df_pos['word'] = df_pos['word'].str.lower()                                 # make all the words lowercase for a more fair count print(df_pos.sample(n=5, random_state=5))print(f'dataframe entries: {len(df_pos)}')          word pos_tag33998        ?   PUNCT7390      when   SCONJ60175  because   SCONJ58601   amused    VERB61726       it    PRONdataframe entries: 65359And we see that it has almost x10 more entries than if we were storing the data in a dict.Now we can extract some basic insights, for example the distribution of tags along the corpus:df_count_pos = df_pos.groupby('pos_tag')['pos_tag'].count().\\                        reset_index(name='count').sort_values(['count'],ascending=True).reset_index(drop=True)# create histogram with pos_tag distributionfig, ax = plt.subplots(figsize=(16,9))sns.barplot(    data=df_count_pos,    y='pos_tag',    x='count',    palette=COLORS)plt.title('Distribution of Part-of-Speech tags')plt.grid(True)plt.show()Interestingly, we see that pronouns, verbs and nouns dominate in the corpus. This might be a proxy to the short-nature of SMS, since the messages have to be direct and for exmaple contain a high density of ‘I’, ‘you’, ‘we’, …, etc.Another thing we can look at is at the top 10 most frequent adjectives. This might gives us a sense of the overall sentiment of the corpus:df_adj = df_pos[df_pos['pos_tag'] == 'ADJ']df_adj_count = df_adj.groupby('word')['word'].count().\\                        reset_index(name='count').sort_values(['count'],ascending=False).reset_index(drop=True)print(df_adj_count.sort_values('count', ascending=False).head(n=10))    word  count0   good    1361  great     992  sorry     903   sure     904    new     615   much     576  other     557   last     498   more     459   free     44As expected from the WordCloud above, ‘good’ is the most common adjective. Just from this very broad view, we can see that the sentiment of the most used words is quite positive.SpotcheckNow we’d like to see how well our POS approach is performing. For that, we can take the most frequent adjective (good), and see how it compares to the count in the BOW model.df_bow = pd.DataFrame.from_dict({    'word': wcount_dict.keys(),    'count': wcount_dict.values()}).sort_values('count', ascending=False)print(f\"BOW 'good' word counts: {df_bow[df_bow['word'] == 'good']['count'].values[0]}\")print(f\"ADJ 'good' word counts: {df_adj_count[df_adj_count['word'] == 'good']['count'].values[0]}\")# create a spotcheck dataframedf_spk = df_pos[(df_pos['word'] == 'good') &amp; (df_pos['pos_tag'] != 'ADJ')]print(df_spk)print(f'dataframe entries in spotcheck dataframe: {len(df_spk)}')print(f\"# of 'good' entries missing: {df_bow[df_bow['word'] == 'good']['count'].values[0] - df_adj_count[df_adj_count['word'] == 'good']['count'].values[0] - len(df_spk)}\")BOW 'good' word counts: 182ADJ 'good' word counts: 136       word pos_tag3722   good    INTJ4058   good    INTJ9954   good    INTJ13146  good    NOUN13449  good    NOUN13619  good    INTJ15406  good    INTJ16990  good    NOUN29341  good    INTJ31037  good    INTJ33624  good    NOUN51220  good    INTJ51882  good    INTJ62734  good    INTJdataframe entries in spotcheck dataframe: 14# of 'good' entries missing: 32We see that most of the instances of ‘good’ that are not classified as ADJ are classified as INTJ (interjection), which probably corresponds to messages where ‘good’ is the only word (e.g. ‘Good!’ as an answer to an SMS).Also, it looks like there are 60 instances of the word ‘good’ that appear in the BOW model but not in the BOW model. This is most likely due to lemmatization. That is, the spacy.nlp() pipeline removing the ending of a word and transforming it into ‘good’ (e.g. ‘goodbye’, ‘goods’, …). To be more rigorous and test the validity of the en_core_web_sm model for SMS, I would have to do spotchecks for more words and drill down whether this might be due to lemmatization or other causes.Summary &amp; ConclusionsIn this project I’ve used basic NLP tools from the spaCy and sklearn libraries to study the nature of SMS texts on a public dataset. I hope I’ve convinced you that even basic NLP tools are able to extract insights from SMS data. Here are the main take-aways from this analysis:      NLP is able to capture and quantify the short and colloquial nature of SMS: Through vectorization (using the nltk.CountVectorizer() and nltk.TfidfVectorizer() models), the wordclouds show that the most dominant words in the SMS dataset are generic words such as ‘thank’, ‘know’, ‘good’ and colloquial words like ‘lol’, ‘haha’ or ‘hi’.        Classical NLP tools don’t work too well with a variety of languages: Because most of the available models on spacy are trained in one language only (English in the case of en_core_web_sm), it’s difficult to apply a NLP analysis on SMS dataset from countries that have a rich variety of languages. This is what motivated me to only consider SMS from the US (as opposed to Singapore or India).        SMS datasets are extremely sensitive to age bias: As described in Chen, T., Kan, MY. Creating a live, public short message service corpus: the NUS SMS corpus, the age distribution of collected SMS is corresponds mostly to an age group of between 18-24 years old (see figure 3 in the paper). The way people write SMS depends a lot on their demographic (i.e. younger people will write SMS differently than younger people) and therefore the results and conclusions from this analysis are most likely biased towards a younger demographic.        SMS is not really Natural Language: Unlike more formal text-based datasets (e.g. movie reviews or policy documents), SMS datasets are extremely loose in their use of language. Some equivalent words might not be captured by the model to be the same becuase they’re written different (e.g. ‘good’ vs. ‘gooood’) or some abbreviations might not be understood by the model (e.g. ‘lol’). This will affect the results we get from the NLP analysis since SMS is not really Natural Language in the written sense.  Further OpportunitiesI will finish with some final remarks about how one could extend and enhance this analysis:      Language Detection: One interesting question we could ask is: What are the most frequently used words outside of the country’s official language? For example, it’d be interesting to study what words (outside of English) are used most frequently in the USA (I’d expect they’d be Spanish). We could use one of the many Python libraries to detect language (e.g. spacy_langdetect or googletrans), filter out words in the official language(s) of the country, and run a similar vectorisation process as we did here.        Alternative stratification/binning: In this case I binned the data based on the country. I could bin the data based on age, sex or language. The latter would be especially useful given that most available NLP tools in Python cannot handle multilingual datasets. However, age and sex binning wouldn’t be possible in this dataset because the researchers didn’t capture that information. Given that language is heavily influenced by demographics, an alternative binning might give us different results and conclusions.        Sentiment Analysis: In this project I used the POS-tagger to get a very basic sentiment for the USA subset of this SMS dataset. We could use more sophisticated ML models for sentiment analysis, such as the VADER model. However, one should be careful to jump on the ML-train as these models are normally trained in datasets of a different domain. Also, we will sacrifice interpretability as these models tend to be a black box.        Named Entity Recognition: In addition to using a POS-tagger, we could run a Named Entity Recognition (NER) analysis, which uses tags words based named entities such as location or animal.  You can find all the code for this analysis on this GitHub repo. Stay tuned for part III of this series!"
  },
  
  {
    "title": "NLP analysis on SMS text - part I",
    "url": "/posts/nlp-sms-part1/",
    "categories": "data-portfolio",
    "tags": "nlp, data-science",
    "date": "2022-08-03 20:34:00 +1000",
    





    "snippet": "In this project, I’ll be using Natural Language Processing (NLP) to study an SMS dataset. I’m interested to understand whether NLP can tell us something about the nature of SMS.For the chosen dataset, I hope to answer some of the following questions:  What words are used most frequently in English SMS texting?  Is NLP a good tool to study short text like SMS?  What’s the main sentiment(s) of SMS texts as captured by basic NLP tools?  What are some potential sources of bias when studying SMS datasets?In part I of this series I will focus on the pre-processing of the data The main goal of this project is to showcase those who are not NLP experts (like me) the value of basic NLP tools. So let’s dive right in! First, I’ll import some neccesary packages and then have a look at the dataset:from jupyterthemes import jtplotfrom IPython.display import display_htmlfrom itertools import chain,cycleimport seaborn as sns%config InlineBackend.figure_format = 'retina'                                    # so you can see plots in HD :)sns.set(style=\"whitegrid\", font_scale=1.4)sns.set_palette(\"colorblind\")COLORS = sns.color_palette(\"deep\", 12).as_hex()darkmode_on = Trueif darkmode_on:    jtplot.style(theme='grade3', context='talk', ticks=True, grid=True)    def display_side_by_side(*args,titles=cycle([''])):    html_str=''    for df,title in zip(args, chain(titles,cycle(['&lt;/br&gt;'])) ):        html_str+='&lt;th style=\"text-align:center\"&gt;&lt;td style=\"vertical-align:top\"&gt;'        html_str+=f'&lt;h2&gt;{title}&lt;/h2&gt;'        html_str+=df.to_html().replace('table','table style=\"display:inline\"')        html_str+='&lt;/td&gt;&lt;/th&gt;'    display_html(html_str,raw=True)DatasetThe dataset used for this analysis compromises 71,000 messages focusing on English and Mandarin Chinese. The dataset is open sourced (at the time of writing) and available here. The details the dataset are also described in Chen, T., Kan, MY. Creating a live, public short message service corpus: the NUS SMS corpus, authored by the researchers who also collected the data.Let’s quickly have a look at the data:import pandas as pd# extract datasetdf_raw = pd.read_csv('data/clean_nus_sms.csv', index_col=0)df = df_raw.copy()# explore datsetprint(df_raw.keys())df_raw.head()Index(['id', 'Message', 'length', 'country', 'Date'], dtype='object')There are not many columns in the dataset, which makes it easier to stratify it or bin it. We could choose to bin it based on the length or Date columns but given that we’re taking an NLP approach and that language is normally country-depdendant. Because of that, let’s look at the distribution of the data based on the country:import matplotlib.pyplot as pltfig, ax = plt.subplots(1,1, figsize=(12,15))sns.histplot(    data=df,    y='country',    ax=ax,     color=COLORS[0])ax.set_xscale('log')ax.set_title('SMS count per country (LOGSCALE)')ax.set_ylabel('countries')ax.set_xlabel('count')locs, labels = plt.xticks()plt.setp(labels, rotation=0)plt.show()Noticing that the counts (x-axis) in the distribution are in log-scale, we see that we have a lot of data (SMS) for countries like Singapore (&gt;10,000), whereas for most countries we only have around 10 SMS. For this analysis, let’s only consider countries that have at least 1,000 SMS, and consider other bins of data to have an insuficient sample size for our analysis.However, we need to be careful because countries like Singapore or the USA two codes: ‘SG’ and ‘Singapore’. For simplicity and consistency, I’ll change ‘SG’ to ‘Singapore’ and ‘United States’ to ‘USA’. So we need to do a bit of data wrangling:import spacy as sp# change 'SG' to 'Singapore' and 'United states country' to 'USA'mask_sg = df['country'] == 'SG'df.loc[mask_sg, 'country'] = 'Singapore'mask_usa = df['country'] == 'United States'df.loc[mask_usa, 'country'] = 'USA'# group by countrycount_label = 'count_sms'df[count_label] = 1grouped_country = df.groupby(by='country', as_index=False)[count_label].count()# find what countries have a sample size greater than the threshold definedNSAMPLE_THD = 1000valid_countries = [row['country'] for index, row in grouped_country.iterrows() if row[count_label] &gt; NSAMPLE_THD]# filter out countries that have a statistically suficient sample sizedf_temp = df[df['country'].isin(valid_countries)].reset_index(drop=True)data_cols = [    'Message',    'length',    'country',    'Date']data = df_temp[data_cols]data.head()# filter out countries that don't meet criteria in grouped dfgrouped_country = grouped_country[grouped_country['country'].isin(valid_countries)].sort_values(count_label, ascending=True).reset_index(drop=True)# plot filtered distributionfig, ax = plt.subplots(1,1, figsize=(15,10), sharex=True)sns.barplot(    data=grouped_country,    y='country',    x=count_label,    ax=ax,     color=COLORS[0])ax.grid(True)ax.set_ylabel('countries')ax.set_title('SMS count per country (n &gt; 1000 SMS)')plt.show()That looks much neater! Also, it means that the analysis will be more statistically meaningful. One should note however that because the study was performed in Singapore, there’s an oversampling for Singapore datapoints. Given the other populations, we could randomly decimate the data for Singapore, but I won’t be doing that in this analysis.Text Pre-PreprocessingFor this project, the NLP library I chose is spaCy, because it allows one to customize and add components to NLP pipelines. In this context, a pipeline refers to different analytical tools or pre-rpocessing techniques to extract insight from text, such as the [lemmatizer]https://en.wikipedia.org/wiki/Lemmatisation) (which loosely speaking extract the ‘root’ of a word) or the tagger (which assigns part-of-speech tags to the words).Now into the fun part! Before we start using NLP tools, we need to clean our data. In NLP, this means that we’ll need to do some text pre-processing. For example, we might want to remove words that appear very often but are not very insightful, like ‘a’, ‘an’. These words are known as stopwords.Part of the pre-processing also entails building a good data-structure to embed the text data. Initially, I set to build a data-structure for the corpus (text-dataset) that would allow me to keep track of what country each SMS corresponds to. One can achieve that through annotations, and as mentioned before, the great thing about spaCy is that it allows one to customize elements of the nlp() pipeline. In this case, it meant that I added the country as an attribute extension to the Doc object:from spacy.tokens import Docadd_annotations = Falseif add_annotations:    Doc.set_extension('country', default=None, force=True)    corpus = data[[\"Message\",\"country\"]].astype(str).apply(tuple, axis=1).valuesHowever, upon some exploration I realised that the problem would be more challenging as some of these countries have a rich variety of languages (e.g. Singapore and India):print(    data[data['country'] == 'Singapore'].Message.values[:10], '\\n',    '='*180+'\\n',    data[data['country'] == 'India'].Message.values[:10], '\\n',    '='*180)['Bugis oso near wat...' 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...' 'I dunno until when... Lets go learn pilates...' 'Den only weekdays got special price... Haiz... Cant eat liao... Cut nails oso muz wait until i finish drivin wat, lunch still muz eat wat...' 'Meet after lunch la...' 'm walking in citylink now ü faster come down... Me very hungry...' '5 nights...We nt staying at port step liao...Too ex' 'Hey pple...$700 or $900 for 5 nights...Excellent location wif breakfast hamper!!!' 'Yun ah.the ubi one say if ü wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.èn' 'Hey tmr maybe can meet you at yck']  ==================================================================================================================================================================================== ['K' 'Studying?' 'Vch photo' 'K:-)ya i hav to finish' 'One senioq akka' 'K d' 'She vil mistake me only cha.dnt talk to me also' 'I am standing up' 'Sorry d v seriously forgot' 'Free']  ====================================================================================================================================================================================An English-trained model like en_core_web_sm won’t perform well on corpa with a diversity of languages, thus any insights drawn from the results would be biased to English-text patterns. So I decided to build a pre-processing pipeline for USA messages and then re-use the the pipeline for other countries on a different project.The main point of this pre-processing pipeline is to: (i) get rid of stopwords and (ii) extract the lemmas in the text (root of each word that’s still readable).from nltk.corpus import stopwords# extract stopword set and update with more colloquial wordsstop_words = {s.lower() for s in stopwords.words(\"english\")}# load model, get a subsample of the model and extract lemmasnlp = sp.load('en_core_web_sm')def text_preprocessing_pipeline(country):    '''    Find lemmas and pos_tag of a subset of SMS based on    country of origin    '''    country_sms = data[data['country'] == 'USA']    country_docs = nlp.pipe(country_sms['Message'].astype(str))    lemmas, pos_tags = [], []    for doc in country_docs:        lemma_i = [token.lemma_.lower() for token in doc if token.lemma_.isalpha() and token.lemma_.lower() not in stop_words]        if len(lemma_i) == 0:            pass        else:            lemmas.append(\" \".join(lemma_i))                pos_tags_i = {token.text: token.pos_ for token in doc}        pos_tags.append(pos_tags_i)                return lemmas, pos_tags            # run preprocess pipeline for USAlemmas, pos_tags = text_preprocessing_pipeline(country='USA')Next StepsNow that we have pre-processed the dataset (extracting the lemmas and removing stopwords), we can actually move on to analyse the corpus using NLP tools. That will require somehow turning text into numbers and numerical objects that can be analysed and processed more easily. If you’re interested, you can read further on NLP analysis on SMS text - part II!"
  },
  
  {
    "title": "Welcome!",
    "url": "/posts/welcome/",
    "categories": "",
    "tags": "",
    "date": "2022-08-03 09:49:00 +1000",
    





    "snippet": "Hey 👋 welcome to my personal wesbsite and Data Science portfolio! I’ll also aim to blog sporadically here about the data landscape, especially around responsible use of data and ML."
  }
  
]

