[
  
  {
    "title": "NLP analysis on SMS text - part II",
    "url": "/posts/nlp-sms-part2/",
    "categories": "data-portfolio",
    "tags": "nlp, data-science",
    "date": "2022-08-04 20:34:00 +1000",
    





    "snippet": "In the first part of this Natural Language Processing (NLP) series, I pre-processed a SMS dataset using spaCy. In this post, Iâ€™ll use the Python library sklearn to extract some insights from the dataset.Because we have a text-based dataset, we need to turn words into numerical objects to carry out our analysis. This leads us to the concept of vectorisation.VectorisationIn NLP, vectorisation is the process of mapping words in the corpus to numerical features so that we can more easily analyse the dataset mathematically - either by means of a statistical analysis or by feeding the vector into a Machine Learning algorithm. In this post I wonâ€™t be using any Machine Learning Algorithms.Bag-of-Words (BOW)When it comes to vectorising the corpus (text-dataset), the most naive approach is to create a Bag-of-Words through the CountVectorizer() object. This technique counts the number of times a lemma appears in the corpus. Once we have the number of counts for each lemma, we can go ahead and build a wordcloud to visualise what lemmas are most common in these SMS messages. Wordclouds give more weight to words that appear more frequently by scaling them up in size.As mentioned above, weâ€™ll use sklearn to carry out the vectorisation:import numpy as npfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizerfrom wordcloud import WordCloud# create CountVectorizer() object and generate bow matrixcount_vectorizer = CountVectorizer(    lowercase=True,    stop_words='english',     ngram_range=(1,1))bow_matrix = count_vectorizer.fit_transform(lemmas)feature_names_bow = count_vectorizer.get_feature_names_out()# get word count to create a wordcloudword_count = np.asarray(bow_matrix.sum(axis=0)).ravel().tolist()wcount_dict = {word: count for word, count in zip(feature_names_bow, word_count)}def black_color_func(word, font_size, position, orientation, random_state=None, **kwargs):            return(\"hsl(0,100%, 1%)\")def make_wordcloud(    x,     bg_color='white',    cloud_width=3500,     cloud_height=2000,     maxwords=500):        plt.figure(figsize=(16,9))    cloud = WordCloud(        font_path='./data/arial-unicode-ms.ttf',         background_color=bg_color,         width=cloud_width,         height=cloud_height,         max_words=maxwords)    cloud.generate_from_frequencies(x)    cloud.recolor(color_func = black_color_func)    plt.imshow(cloud, interpolation=\"bilinear\")    plt.axis(\"off\")    plt.show()# build wordcloudmake_wordcloud(wcount_dict)Interesting! Some of the words that come up most often across all the corpus are â€˜thankâ€™, â€˜knowâ€™, â€˜goodâ€™, â€˜comeâ€™, â€˜nightâ€™, â€¦ This is quite intuitive as these seem very common words one would use in short-format messaging (e.g. â€˜thank youâ€™, â€˜Iâ€™m comingâ€™, â€˜I think soâ€™, â€¦)Term Frequency Inverse-Document Frequency (TF-IDF)A less naive approach to capture important words is the Term Frequency Inverse-Document Frequency (tf-idf). Unlike a classical bag-of-words approach, tf-idf takes into account the number of documents in the corpus that contain each words. This helps highlight more special words that are common only in very few documents, and weighs down very common words across all documents. For more information on how TF-IDF is calculated, see its Wikipedia article here.# create TfidfVectorizer() object and generate tfidf sparse matrixtfidf_vectorizer = TfidfVectorizer(    stop_words='english',    lowercase=True)tfidf_matrix = tfidf_vectorizer.fit_transform(lemmas)feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()# get word count to create a wordcloudweight_count = np.asarray(tfidf_matrix.sum(axis=0)).ravel().tolist()tfidf_count = np.array(weight_count)*np.array(word_count)if np.all(feature_names_tfidf == feature_names_bow):    tfidf_dict = {word: count for word, count in zip(feature_names_tfidf, tfidf_count)}# build tfidf wordcloudmake_wordcloud(tfidf_dict)Very similar to the BOW approach! We can see that some words like â€˜hiâ€™ become less important while other words like â€˜yeahâ€™ and â€˜lolâ€™ become more relevant, which is surprising as Iâ€™d expected these words to still be very frequent across all SMS.Part-of-Speech (POS) taggingMaking use of the spacy.nlp() pipeline which we created in part I, we can do more sophisticated analysis like Part-of-Speech tagging (POS). Loosely speaking, POS can be described as the process of capturing context in a corpus by labelling the words in the corpus according to the element of speech they belong to (noun, adjective, adverb, â€¦).The pos tags were extracted and computed while pre-processing the data, but we can make a dataframe to make it clearer. Originally, I set out to build the df_pos in the following way:pos_dict = {word:tag for d_i in pos_tags for word, tag in d_i.items()}df_pos = pd.DataFrame.from_dict({    'word': pos_dict.keys(),    'pos_tag': pos_dict.values()})print(df_pos.sample(n=5, random_state=5))print(f'dataframe entries: {len(df_pos)}')         word pos_tag4654      fee    NOUN942    repair    NOUN2650  himself    PRON1104    spent    VERB4361     goot    NOUNdataframe entries: 7888However, this wonâ€™t work because dictionaries canâ€™t have duplicate keys, so our dataframe will only have one entry per fixed word. Instead, we can use a list of tuples to create the POS dataframe:pos_data = [(word,tag) for d_i in pos_tags for word, tag in d_i.items()]df_pos = pd.DataFrame(pos_data, columns=['word', 'pos_tag'])df_pos['word'] = df_pos['word'].str.lower()                                 # make all the words lowercase for a more fair count print(df_pos.sample(n=5, random_state=5))print(f'dataframe entries: {len(df_pos)}')          word pos_tag33998        ?   PUNCT7390      when   SCONJ60175  because   SCONJ58601   amused    VERB61726       it    PRONdataframe entries: 65359And we see that it has almost x10 more entries than if we were storing the data in a dict.Now we can extract some basic insights, for example the distribution of tags along the corpus:df_count_pos = df_pos.groupby('pos_tag')['pos_tag'].count().\\                        reset_index(name='count').sort_values(['count'],ascending=True).reset_index(drop=True)# create histogram with pos_tag distributionfig, ax = plt.subplots(figsize=(16,9))sns.barplot(    data=df_count_pos,    y='pos_tag',    x='count',    palette=COLORS)plt.title('Distribution of Part-of-Speech tags')plt.grid(True)plt.show()Interestingly, we see that pronouns, verbs and nouns dominate in the corpus. This might be a proxy to the short-nature of SMS, since the messages have to be direct and for exmaple contain a high density of â€˜Iâ€™, â€˜youâ€™, â€˜weâ€™, â€¦, etc.Another thing we can look at is at the top 10 most frequent adjectives. This might gives us a sense of the overall sentiment of the corpus:df_adj = df_pos[df_pos['pos_tag'] == 'ADJ']df_adj_count = df_adj.groupby('word')['word'].count().\\                        reset_index(name='count').sort_values(['count'],ascending=False).reset_index(drop=True)print(df_adj_count.sort_values('count', ascending=False).head(n=10))    word  count0   good    1361  great     992  sorry     903   sure     904    new     615   much     576  other     557   last     498   more     459   free     44As expected from the WordCloud above, â€˜goodâ€™ is the most common adjective. Just from this very broad view, we can see that the sentiment of the most used words is quite positive.SpotcheckNow weâ€™d like to see how well our POS approach is performing. For that, we can take the most frequent adjective (good), and see how it compares to the count in the BOW model.df_bow = pd.DataFrame.from_dict({    'word': wcount_dict.keys(),    'count': wcount_dict.values()}).sort_values('count', ascending=False)print(f\"BOW 'good' word counts: {df_bow[df_bow['word'] == 'good']['count'].values[0]}\")print(f\"ADJ 'good' word counts: {df_adj_count[df_adj_count['word'] == 'good']['count'].values[0]}\")# create a spotcheck dataframedf_spk = df_pos[(df_pos['word'] == 'good') &amp; (df_pos['pos_tag'] != 'ADJ')]print(df_spk)print(f'dataframe entries in spotcheck dataframe: {len(df_spk)}')print(f\"# of 'good' entries missing: {df_bow[df_bow['word'] == 'good']['count'].values[0] - df_adj_count[df_adj_count['word'] == 'good']['count'].values[0] - len(df_spk)}\")BOW 'good' word counts: 182ADJ 'good' word counts: 136       word pos_tag3722   good    INTJ4058   good    INTJ9954   good    INTJ13146  good    NOUN13449  good    NOUN13619  good    INTJ15406  good    INTJ16990  good    NOUN29341  good    INTJ31037  good    INTJ33624  good    NOUN51220  good    INTJ51882  good    INTJ62734  good    INTJdataframe entries in spotcheck dataframe: 14# of 'good' entries missing: 32We see that most of the instances of â€˜goodâ€™ that are not classified as ADJ are classified as INTJ (interjection), which probably corresponds to messages where â€˜goodâ€™ is the only word (e.g. â€˜Good!â€™ as an answer to an SMS).Also, it looks like there are 60 instances of the word â€˜goodâ€™ that appear in the BOW model but not in the BOW model. This is most likely due to lemmatization. That is, the spacy.nlp() pipeline removing the ending of a word and transforming it into â€˜goodâ€™ (e.g. â€˜goodbyeâ€™, â€˜goodsâ€™, â€¦). To be more rigorous and test the validity of the en_core_web_sm model for SMS, I would have to do spotchecks for more words and drill down whether this might be due to lemmatization or other causes.Summary &amp; ConclusionsIn this project Iâ€™ve used basic NLP tools from the spaCy and sklearn libraries to study the nature of SMS texts on a public dataset. I hope Iâ€™ve convinced you that even basic NLP tools are able to extract insights from SMS data. Here are the main take-aways from this analysis:      NLP is able to capture and quantify the short and colloquial nature of SMS: Through vectorization (using the nltk.CountVectorizer() and nltk.TfidfVectorizer() models), the wordclouds show that the most dominant words in the SMS dataset are generic words such as â€˜thankâ€™, â€˜knowâ€™, â€˜goodâ€™ and colloquial words like â€˜lolâ€™, â€˜hahaâ€™ or â€˜hiâ€™.        Classical NLP tools donâ€™t work too well with a variety of languages: Because most of the available models on spacy are trained in one language only (English in the case of en_core_web_sm), itâ€™s difficult to apply a NLP analysis on SMS dataset from countries that have a rich variety of languages. This is what motivated me to only consider SMS from the US (as opposed to Singapore or India).        SMS datasets are extremely sensitive to age bias: As described in Chen, T., Kan, MY. Creating a live, public short message service corpus: the NUS SMS corpus, the age distribution of collected SMS is corresponds mostly to an age group of between 18-24 years old (see figure 3 in the paper). The way people write SMS depends a lot on their demographic (i.e. younger people will write SMS differently than younger people) and therefore the results and conclusions from this analysis are most likely biased towards a younger demographic.        SMS is not really Natural Language: Unlike more formal text-based datasets (e.g. movie reviews or policy documents), SMS datasets are extremely loose in their use of language. Some equivalent words might not be captured by the model to be the same becuase theyâ€™re written different (e.g. â€˜goodâ€™ vs. â€˜goooodâ€™) or some abbreviations might not be understood by the model (e.g. â€˜lolâ€™). This will affect the results we get from the NLP analysis since SMS is not really Natural Language in the written sense.  Further OpportunitiesI will finish with some final remarks about how one could extend and enhance this analysis:      Language Detection: One interesting question we could ask is: What are the most frequently used words outside of the countryâ€™s official language? For example, itâ€™d be interesting to study what words (outside of English) are used most frequently in the USA (Iâ€™d expect theyâ€™d be Spanish). We could use one of the many Python libraries to detect language (e.g. spacy_langdetect or googletrans), filter out words in the official language(s) of the country, and run a similar vectorisation process as we did here.        Alternative stratification/binning: In this case I binned the data based on the country. I could bin the data based on age, sex or language. The latter would be especially useful given that most available NLP tools in Python cannot handle multilingual datasets. However, age and sex binning wouldnâ€™t be possible in this dataset because the researchers didnâ€™t capture that information. Given that language is heavily influenced by demographics, an alternative binning might give us different results and conclusions.        Sentiment Analysis: In this project I used the POS-tagger to get a very basic sentiment for the USA subset of this SMS dataset. We could use more sophisticated ML models for sentiment analysis, such as the VADER model. However, one should be careful to jump on the ML-train as these models are normally trained in datasets of a different domain. Also, we will sacrifice interpretability as these models tend to be a black box.        Named Entity Recognition: In addition to using a POS-tagger, we could run a Named Entity Recognition (NER) analysis, which uses tags words based named entities such as location or animal.  You can find all the code for this analysis on this GitHub repo. Stay tuned for part III of this series!"
  },
  
  {
    "title": "NLP analysis on SMS text - part I",
    "url": "/posts/nlp-sms-part1/",
    "categories": "data-portfolio",
    "tags": "nlp, data-science",
    "date": "2022-08-03 20:34:00 +1000",
    





    "snippet": "In this project, Iâ€™ll be using Natural Language Processing (NLP) to study an SMS dataset. Iâ€™m interested to understand whether NLP can tell us something about the nature of SMS.For the chosen dataset, I hope to answer some of the following questions:  What words are used most frequently in English SMS texting?  Is NLP a good tool to study short text like SMS?  Whatâ€™s the main sentiment(s) of SMS texts as captured by basic NLP tools?  What are some potential sources of bias when studying SMS datasets?In part I of this series I will focus on the pre-processing of the data The main goal of this project is to showcase those who are not NLP experts (like me) the value of basic NLP tools. So letâ€™s dive right in! First, Iâ€™ll import some neccesary packages and then have a look at the dataset:from jupyterthemes import jtplotfrom IPython.display import display_htmlfrom itertools import chain,cycleimport seaborn as sns%config InlineBackend.figure_format = 'retina'                                    # so you can see plots in HD :)sns.set(style=\"whitegrid\", font_scale=1.4)sns.set_palette(\"colorblind\")COLORS = sns.color_palette(\"deep\", 12).as_hex()darkmode_on = Trueif darkmode_on:    jtplot.style(theme='grade3', context='talk', ticks=True, grid=True)    def display_side_by_side(*args,titles=cycle([''])):    html_str=''    for df,title in zip(args, chain(titles,cycle(['&lt;/br&gt;'])) ):        html_str+='&lt;th style=\"text-align:center\"&gt;&lt;td style=\"vertical-align:top\"&gt;'        html_str+=f'&lt;h2&gt;{title}&lt;/h2&gt;'        html_str+=df.to_html().replace('table','table style=\"display:inline\"')        html_str+='&lt;/td&gt;&lt;/th&gt;'    display_html(html_str,raw=True)DatasetThe dataset used for this analysis compromises 71,000 messages focusing on English and Mandarin Chinese. The dataset is open sourced (at the time of writing) and available here. The details the dataset are also described in Chen, T., Kan, MY. Creating a live, public short message service corpus: the NUS SMS corpus, authored by the researchers who also collected the data.Letâ€™s quickly have a look at the data:import pandas as pd# extract datasetdf_raw = pd.read_csv('data/clean_nus_sms.csv', index_col=0)df = df_raw.copy()# explore datsetprint(df_raw.keys())df_raw.head()Index(['id', 'Message', 'length', 'country', 'Date'], dtype='object')There are not many columns in the dataset, which makes it easier to stratify it or bin it. We could choose to bin it based on the length or Date columns but given that weâ€™re taking an NLP approach and that language is normally country-depdendant. Because of that, letâ€™s look at the distribution of the data based on the country:import matplotlib.pyplot as pltfig, ax = plt.subplots(1,1, figsize=(12,15))sns.histplot(    data=df,    y='country',    ax=ax,     color=COLORS[0])ax.set_xscale('log')ax.set_title('SMS count per country (LOGSCALE)')ax.set_ylabel('countries')ax.set_xlabel('count')locs, labels = plt.xticks()plt.setp(labels, rotation=0)plt.show()Noticing that the counts (x-axis) in the distribution are in log-scale, we see that we have a lot of data (SMS) for countries like Singapore (&gt;10,000), whereas for most countries we only have around 10 SMS. For this analysis, letâ€™s only consider countries that have at least 1,000 SMS, and consider other bins of data to have an insuficient sample size for our analysis.However, we need to be careful because countries like Singapore or the USA two codes: â€˜SGâ€™ and â€˜Singaporeâ€™. For simplicity and consistency, Iâ€™ll change â€˜SGâ€™ to â€˜Singaporeâ€™ and â€˜United Statesâ€™ to â€˜USAâ€™. So we need to do a bit of data wrangling:import spacy as sp# change 'SG' to 'Singapore' and 'United states country' to 'USA'mask_sg = df['country'] == 'SG'df.loc[mask_sg, 'country'] = 'Singapore'mask_usa = df['country'] == 'United States'df.loc[mask_usa, 'country'] = 'USA'# group by countrycount_label = 'count_sms'df[count_label] = 1grouped_country = df.groupby(by='country', as_index=False)[count_label].count()# find what countries have a sample size greater than the threshold definedNSAMPLE_THD = 1000valid_countries = [row['country'] for index, row in grouped_country.iterrows() if row[count_label] &gt; NSAMPLE_THD]# filter out countries that have a statistically suficient sample sizedf_temp = df[df['country'].isin(valid_countries)].reset_index(drop=True)data_cols = [    'Message',    'length',    'country',    'Date']data = df_temp[data_cols]data.head()# filter out countries that don't meet criteria in grouped dfgrouped_country = grouped_country[grouped_country['country'].isin(valid_countries)].sort_values(count_label, ascending=True).reset_index(drop=True)# plot filtered distributionfig, ax = plt.subplots(1,1, figsize=(15,10), sharex=True)sns.barplot(    data=grouped_country,    y='country',    x=count_label,    ax=ax,     color=COLORS[0])ax.grid(True)ax.set_ylabel('countries')ax.set_title('SMS count per country (n &gt; 1000 SMS)')plt.show()That looks much neater! Also, it means that the analysis will be more statistically meaningful. One should note however that because the study was performed in Singapore, thereâ€™s an oversampling for Singapore datapoints. Given the other populations, we could randomly decimate the data for Singapore, but I wonâ€™t be doing that in this analysis.Text Pre-PreprocessingFor this project, the NLP library I chose is spaCy, because it allows one to customize and add components to NLP pipelines. In this context, a pipeline refers to different analytical tools or pre-rpocessing techniques to extract insight from text, such as the [lemmatizer]https://en.wikipedia.org/wiki/Lemmatisation) (which loosely speaking extract the â€˜rootâ€™ of a word) or the tagger (which assigns part-of-speech tags to the words).Now into the fun part! Before we start using NLP tools, we need to clean our data. In NLP, this means that weâ€™ll need to do some text pre-processing. For example, we might want to remove words that appear very often but are not very insightful, like â€˜aâ€™, â€˜anâ€™. These words are known as stopwords.Part of the pre-processing also entails building a good data-structure to embed the text data. Initially, I set to build a data-structure for the corpus (text-dataset) that would allow me to keep track of what country each SMS corresponds to. One can achieve that through annotations, and as mentioned before, the great thing about spaCy is that it allows one to customize elements of the nlp() pipeline. In this case, it meant that I added the country as an attribute extension to the Doc object:from spacy.tokens import Docadd_annotations = Falseif add_annotations:    Doc.set_extension('country', default=None, force=True)    corpus = data[[\"Message\",\"country\"]].astype(str).apply(tuple, axis=1).valuesHowever, upon some exploration I realised that the problem would be more challenging as some of these countries have a rich variety of languages (e.g. Singapore and India):print(    data[data['country'] == 'Singapore'].Message.values[:10], '\\n',    '='*180+'\\n',    data[data['country'] == 'India'].Message.values[:10], '\\n',    '='*180)['Bugis oso near wat...' 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...' 'I dunno until when... Lets go learn pilates...' 'Den only weekdays got special price... Haiz... Cant eat liao... Cut nails oso muz wait until i finish drivin wat, lunch still muz eat wat...' 'Meet after lunch la...' 'm walking in citylink now Ã¼ faster come down... Me very hungry...' '5 nights...We nt staying at port step liao...Too ex' 'Hey pple...$700 or $900 for 5 nights...Excellent location wif breakfast hamper!!!' 'Yun ah.the ubi one say if Ã¼ wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.Ã¨n' 'Hey tmr maybe can meet you at yck']  ==================================================================================================================================================================================== ['K' 'Studying?' 'Vch photo' 'K:-)ya i hav to finish' 'One senioq akka' 'K d' 'She vil mistake me only cha.dnt talk to me also' 'I am standing up' 'Sorry d v seriously forgot' 'Free']  ====================================================================================================================================================================================An English-trained model like en_core_web_sm wonâ€™t perform well on corpa with a diversity of languages, thus any insights drawn from the results would be biased to English-text patterns. So I decided to build a pre-processing pipeline for USA messages and then re-use the the pipeline for other countries on a different project.The main point of this pre-processing pipeline is to: (i) get rid of stopwords and (ii) extract the lemmas in the text (root of each word thatâ€™s still readable).from nltk.corpus import stopwords# extract stopword set and update with more colloquial wordsstop_words = {s.lower() for s in stopwords.words(\"english\")}# load model, get a subsample of the model and extract lemmasnlp = sp.load('en_core_web_sm')def text_preprocessing_pipeline(country):    '''    Find lemmas and pos_tag of a subset of SMS based on    country of origin    '''    country_sms = data[data['country'] == 'USA']    country_docs = nlp.pipe(country_sms['Message'].astype(str))    lemmas, pos_tags = [], []    for doc in country_docs:        lemma_i = [token.lemma_.lower() for token in doc if token.lemma_.isalpha() and token.lemma_.lower() not in stop_words]        if len(lemma_i) == 0:            pass        else:            lemmas.append(\" \".join(lemma_i))                pos_tags_i = {token.text: token.pos_ for token in doc}        pos_tags.append(pos_tags_i)                return lemmas, pos_tags            # run preprocess pipeline for USAlemmas, pos_tags = text_preprocessing_pipeline(country='USA')Next StepsNow that we have pre-processed the dataset (extracting the lemmas and removing stopwords), we can actually move on to analyse the corpus using NLP tools. That will require somehow turning text into numbers and numerical objects that can be analysed and processed more easily. If youâ€™re interested, you can read further on NLP analysis on SMS text - part II!"
  },
  
  {
    "title": "Welcome!",
    "url": "/posts/welcome/",
    "categories": "",
    "tags": "",
    "date": "2022-08-03 09:49:00 +1000",
    





    "snippet": "Hey ðŸ‘‹ welcome to my personal wesbsite and Data Science portfolio! Iâ€™ll also aim to blog sporadically here about the data landscape, especially around responsible use of data and ML."
  }
  
]

