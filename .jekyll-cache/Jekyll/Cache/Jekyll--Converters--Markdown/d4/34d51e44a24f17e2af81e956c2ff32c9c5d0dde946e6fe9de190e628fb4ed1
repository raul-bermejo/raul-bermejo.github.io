I"ˇê<p>In the <a href="/posts/nlp-sms-part1/">first part of this Natural Language Processing (NLP) series</a>, I pre-processed a SMS dataset using <code class="language-plaintext highlighter-rouge">spaCy</code>. In this post, I‚Äôll use the Python library <code class="language-plaintext highlighter-rouge">sklearn</code> to extract some insights from the dataset.</p>

<p>Because we have a text-based dataset, we need to turn words into numerical objects to carry out our analysis. This leads us to the concept of vectorisation.</p>

<h2 id="vectorisation">Vectorisation</h2>

<p>In NLP, vectorisation is the process of mapping words in the corpus to numerical features so that we can more easily analyse the dataset mathematically - either by means of a statistical analysis or by feeding the vector into a Machine Learning algorithm. In this post I won‚Äôt be using any Machine Learning Algorithms.</p>

<h3 id="bag-of-words-bow">Bag-of-Words (BOW)</h3>

<p>When it comes to vectorising the corpus (text-dataset), the most naive approach is to create a Bag-of-Words through the <code class="language-plaintext highlighter-rouge">CountVectorizer()</code> object. This technique counts the number of times a lemma appears in the corpus. Once we have the number of counts for each lemma, we can go ahead and build a <em>wordcloud</em> to visualise what lemmas are most common in these SMS messages. Wordclouds give more weight to words that appear more frequently by scaling them up in size.</p>

<p>As mentioned above, we‚Äôll use <code class="language-plaintext highlighter-rouge">sklearn</code> to carry out the vectorisation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td> --><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span><span class="p">,</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">wordcloud</span> <span class="kn">import</span> <span class="n">WordCloud</span>

<span class="c1"># create CountVectorizer() object and generate bow matrix
</span><span class="n">count_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span>
    <span class="n">lowercase</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">stop_words</span><span class="o">=</span><span class="s">'english'</span><span class="p">,</span> 
    <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">bow_matrix</span> <span class="o">=</span> <span class="n">count_vectorizer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">lemmas</span><span class="p">)</span>
<span class="n">feature_names_bow</span> <span class="o">=</span> <span class="n">count_vectorizer</span><span class="p">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>

<span class="c1"># get word count to create a wordcloud
</span><span class="n">word_count</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">bow_matrix</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)).</span><span class="n">ravel</span><span class="p">().</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">wcount_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">count</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">feature_names_bow</span><span class="p">,</span> <span class="n">word_count</span><span class="p">)}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td> --><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">black_color_func</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">font_size</span><span class="p">,</span> <span class="n">position</span><span class="p">,</span> <span class="n">orientation</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">return</span><span class="p">(</span><span class="s">"hsl(0,100%, 1%)"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">make_wordcloud</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span> 
    <span class="n">bg_color</span><span class="o">=</span><span class="s">'white'</span><span class="p">,</span>
    <span class="n">cloud_width</span><span class="o">=</span><span class="mi">3500</span><span class="p">,</span> 
    <span class="n">cloud_height</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> 
    <span class="n">maxwords</span><span class="o">=</span><span class="mi">500</span>
<span class="p">):</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
    <span class="n">cloud</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span>
        <span class="n">font_path</span><span class="o">=</span><span class="s">'./data/arial-unicode-ms.ttf'</span><span class="p">,</span> 
        <span class="n">background_color</span><span class="o">=</span><span class="n">bg_color</span><span class="p">,</span> 
        <span class="n">width</span><span class="o">=</span><span class="n">cloud_width</span><span class="p">,</span> 
        <span class="n">height</span><span class="o">=</span><span class="n">cloud_height</span><span class="p">,</span> 
        <span class="n">max_words</span><span class="o">=</span><span class="n">maxwords</span><span class="p">)</span>
    <span class="n">cloud</span><span class="p">.</span><span class="n">generate_from_frequencies</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">cloud</span><span class="p">.</span><span class="n">recolor</span><span class="p">(</span><span class="n">color_func</span> <span class="o">=</span> <span class="n">black_color_func</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cloud</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">"bilinear"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">"off"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># build wordcloud
</span><span class="n">make_wordcloud</span><span class="p">(</span><span class="n">wcount_dict</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="../../assets/img/nlp/output_18_0.png" alt="png" /></p>

<p>Interesting! Some of the words that come up most often across all the corpus are ‚Äòthank‚Äô, ‚Äòknow‚Äô, ‚Äògood‚Äô, ‚Äòcome‚Äô, ‚Äònight‚Äô, ‚Ä¶ This is quite intuitive as these seem very common words one would use in short-format messaging (e.g. ‚Äòthank you‚Äô, ‚ÄòI‚Äôm coming‚Äô, ‚ÄòI think so‚Äô, ‚Ä¶)</p>

<h3 id="term-frequency-inverse-document-frequency-tf-idf">Term Frequency Inverse-Document Frequency (TF-IDF)</h3>

<p>A less naive approach to capture important words is the Term Frequency Inverse-Document Frequency (tf-idf). Unlike a classical bag-of-words approach, tf-idf takes into account the number of documents in the corpus that contain each words. This helps highlight more special words that are common only in very few documents, and weighs down very common words across all documents. For more information on how TF-IDF is calculated, see its Wikipedia article <a href="(https://en.wikipedia.org/wiki/Tf%E2%80%93idf)">here</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td> --><td class="rouge-code"><pre><span class="c1"># create TfidfVectorizer() object and generate tfidf sparse matrix
</span><span class="n">tfidf_vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span>
    <span class="n">stop_words</span><span class="o">=</span><span class="s">'english'</span><span class="p">,</span>
    <span class="n">lowercase</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">tfidf_matrix</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">lemmas</span><span class="p">)</span>
<span class="n">feature_names_tfidf</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="p">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>

<span class="c1"># get word count to create a wordcloud
</span><span class="n">weight_count</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">tfidf_matrix</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)).</span><span class="n">ravel</span><span class="p">().</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">tfidf_count</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">weight_count</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">word_count</span><span class="p">)</span>

<span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="nb">all</span><span class="p">(</span><span class="n">feature_names_tfidf</span> <span class="o">==</span> <span class="n">feature_names_bow</span><span class="p">):</span>
    <span class="n">tfidf_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">count</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">feature_names_tfidf</span><span class="p">,</span> <span class="n">tfidf_count</span><span class="p">)}</span>


<span class="c1"># build tfidf wordcloud
</span><span class="n">make_wordcloud</span><span class="p">(</span><span class="n">tfidf_dict</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="../../assets/img/nlp/output_22_0.png" alt="png" /></p>

<p>Very similar to the BOW approach! We can see that some words like ‚Äòhi‚Äô become less important while other words like ‚Äòyeah‚Äô and ‚Äòlol‚Äô become more relevant, which is surprising as I‚Äôd expected these words to still be very frequent across all SMS.</p>

<h2 id="part-of-speech-pos-tagging">Part-of-Speech (POS) tagging</h2>

<p>Making use of the <code class="language-plaintext highlighter-rouge">spacy.nlp()</code> pipeline which we created in <a href="/posts/nlp-sms-part1/">part I</a>, we can do more sophisticated analysis like <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">Part-of-Speech tagging (POS)</a>. Loosely speaking, POS can be described as the process of capturing context in a corpus by labelling the words in the corpus according to the element of speech they belong to (noun, adjective, adverb, ‚Ä¶).</p>

<p>The pos tags were extracted and computed while pre-processing the data, but we can make a dataframe to make it clearer. Originally, I set out to build the <code class="language-plaintext highlighter-rouge">df_pos</code> in the following way:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td> --><td class="rouge-code"><pre><span class="n">pos_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span><span class="n">tag</span> <span class="k">for</span> <span class="n">d_i</span> <span class="ow">in</span> <span class="n">pos_tags</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">d_i</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">df_pos</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s">'word'</span><span class="p">:</span> <span class="n">pos_dict</span><span class="p">.</span><span class="n">keys</span><span class="p">(),</span>
    <span class="s">'pos_tag'</span><span class="p">:</span> <span class="n">pos_dict</span><span class="p">.</span><span class="n">values</span><span class="p">()</span>
<span class="p">})</span>

<span class="k">print</span><span class="p">(</span><span class="n">df_pos</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'dataframe entries: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df_pos</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td> --><td class="rouge-code"><pre>         word pos_tag
4654      fee    NOUN
942    repair    NOUN
2650  himself    PRON
1104    spent    VERB
4361     goot    NOUN
dataframe entries: 7888
</pre></td></tr></tbody></table></code></pre></div></div>

<p>However, this won‚Äôt work because dictionaries can‚Äôt have duplicate keys, so our dataframe will only have one entry per fixed word. Instead, we can use a list of tuples to create the POS dataframe:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td> --><td class="rouge-code"><pre><span class="n">pos_data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">,</span><span class="n">tag</span><span class="p">)</span> <span class="k">for</span> <span class="n">d_i</span> <span class="ow">in</span> <span class="n">pos_tags</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">d_i</span><span class="p">.</span><span class="n">items</span><span class="p">()]</span>
<span class="n">df_pos</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pos_data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'word'</span><span class="p">,</span> <span class="s">'pos_tag'</span><span class="p">])</span>
<span class="n">df_pos</span><span class="p">[</span><span class="s">'word'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_pos</span><span class="p">[</span><span class="s">'word'</span><span class="p">].</span><span class="nb">str</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span>                                 <span class="c1"># make all the words lowercase for a more fair count 
</span>
<span class="k">print</span><span class="p">(</span><span class="n">df_pos</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'dataframe entries: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df_pos</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td> --><td class="rouge-code"><pre>          word pos_tag
33998        ?   PUNCT
7390      when   SCONJ
60175  because   SCONJ
58601   amused    VERB
61726       it    PRON
dataframe entries: 65359
</pre></td></tr></tbody></table></code></pre></div></div>

<p>And we see that it has almost x10 more entries than if we were storing the data in a <code class="language-plaintext highlighter-rouge">dict</code>.</p>

<p>Now we can extract some basic insights, for example the distribution of tags along the corpus:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td> --><td class="rouge-code"><pre><span class="n">df_count_pos</span> <span class="o">=</span> <span class="n">df_pos</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'pos_tag'</span><span class="p">)[</span><span class="s">'pos_tag'</span><span class="p">].</span><span class="n">count</span><span class="p">().</span>\
                        <span class="n">reset_index</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'count'</span><span class="p">).</span><span class="n">sort_values</span><span class="p">([</span><span class="s">'count'</span><span class="p">],</span><span class="n">ascending</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># create histogram with pos_tag distribution
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">df_count_pos</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s">'pos_tag'</span><span class="p">,</span>
    <span class="n">x</span><span class="o">=</span><span class="s">'count'</span><span class="p">,</span>
    <span class="n">palette</span><span class="o">=</span><span class="n">COLORS</span>
<span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Distribution of Part-of-Speech tags'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="../../assets/img/nlp/output_30_0.png" alt="png" /></p>

<p>Interestingly, we see that pronouns, verbs and nouns dominate in the corpus. This might be a proxy to the short-nature of SMS, since the messages have to be direct and for exmaple contain a high density of ‚ÄòI‚Äô, ‚Äòyou‚Äô, ‚Äòwe‚Äô, ‚Ä¶, etc.</p>

<p>Another thing we can look at is at the top 10 most frequent adjectives. This might gives us a sense of the overall sentiment of the corpus:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td> --><td class="rouge-code"><pre><span class="n">df_adj</span> <span class="o">=</span> <span class="n">df_pos</span><span class="p">[</span><span class="n">df_pos</span><span class="p">[</span><span class="s">'pos_tag'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'ADJ'</span><span class="p">]</span>

<span class="n">df_adj_count</span> <span class="o">=</span> <span class="n">df_adj</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'word'</span><span class="p">)[</span><span class="s">'word'</span><span class="p">].</span><span class="n">count</span><span class="p">().</span>\
                        <span class="n">reset_index</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'count'</span><span class="p">).</span><span class="n">sort_values</span><span class="p">([</span><span class="s">'count'</span><span class="p">],</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">df_adj_count</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s">'count'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td> --><td class="rouge-code"><pre>    word  count
0   good    136
1  great     99
2  sorry     90
3   sure     90
4    new     61
5   much     57
6  other     55
7   last     49
8   more     45
9   free     44
</pre></td></tr></tbody></table></code></pre></div></div>

<p>As expected from the <code class="language-plaintext highlighter-rouge">WordCloud</code> above, ‚Äògood‚Äô is the most common adjective. Just from this very broad view, we can see that the sentiment of the most used words is quite positive.</p>

<h3 id="spotcheck">Spotcheck</h3>

<p>Now we‚Äôd like to see how well our POS approach is performing. For that, we can take the most frequent adjective (good), and see how it compares to the count in the BOW model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td> --><td class="rouge-code"><pre><span class="n">df_bow</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s">'word'</span><span class="p">:</span> <span class="n">wcount_dict</span><span class="p">.</span><span class="n">keys</span><span class="p">(),</span>
    <span class="s">'count'</span><span class="p">:</span> <span class="n">wcount_dict</span><span class="p">.</span><span class="n">values</span><span class="p">()</span>
<span class="p">}).</span><span class="n">sort_values</span><span class="p">(</span><span class="s">'count'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"BOW 'good' word counts: </span><span class="si">{</span><span class="n">df_bow</span><span class="p">[</span><span class="n">df_bow</span><span class="p">[</span><span class="s">'word'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'good'</span><span class="p">][</span><span class="s">'count'</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"ADJ 'good' word counts: </span><span class="si">{</span><span class="n">df_adj_count</span><span class="p">[</span><span class="n">df_adj_count</span><span class="p">[</span><span class="s">'word'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'good'</span><span class="p">][</span><span class="s">'count'</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># create a spotcheck dataframe
</span><span class="n">df_spk</span> <span class="o">=</span> <span class="n">df_pos</span><span class="p">[(</span><span class="n">df_pos</span><span class="p">[</span><span class="s">'word'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'good'</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df_pos</span><span class="p">[</span><span class="s">'pos_tag'</span><span class="p">]</span> <span class="o">!=</span> <span class="s">'ADJ'</span><span class="p">)]</span>

<span class="k">print</span><span class="p">(</span><span class="n">df_spk</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'dataframe entries in spotcheck dataframe: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">df_spk</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"# of 'good' entries missing: </span><span class="si">{</span><span class="n">df_bow</span><span class="p">[</span><span class="n">df_bow</span><span class="p">[</span><span class="s">'word'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'good'</span><span class="p">][</span><span class="s">'count'</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">df_adj_count</span><span class="p">[</span><span class="n">df_adj_count</span><span class="p">[</span><span class="s">'word'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'good'</span><span class="p">][</span><span class="s">'count'</span><span class="p">].</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">df_spk</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td> --><td class="rouge-code"><pre>BOW 'good' word counts: 182
ADJ 'good' word counts: 136
       word pos_tag
3722   good    INTJ
4058   good    INTJ
9954   good    INTJ
13146  good    NOUN
13449  good    NOUN
13619  good    INTJ
15406  good    INTJ
16990  good    NOUN
29341  good    INTJ
31037  good    INTJ
33624  good    NOUN
51220  good    INTJ
51882  good    INTJ
62734  good    INTJ
dataframe entries in spotcheck dataframe: 14
# of 'good' entries missing: 32
</pre></td></tr></tbody></table></code></pre></div></div>

<p>We see that most of the instances of ‚Äògood‚Äô that are not classified as <code class="language-plaintext highlighter-rouge">ADJ</code> are classified as <code class="language-plaintext highlighter-rouge">INTJ</code> (interjection), which probably corresponds to messages where ‚Äògood‚Äô is the only word (e.g. ‚ÄòGood!‚Äô as an answer to an SMS).</p>

<p>Also, it looks like there are 60 instances of the word ‚Äògood‚Äô that appear in the BOW model but not in the BOW model. This is most likely due to lemmatization. That is, the <code class="language-plaintext highlighter-rouge">spacy.nlp()</code> pipeline removing the ending of a word and transforming it into ‚Äògood‚Äô (e.g. ‚Äògoodbye‚Äô, ‚Äògoods‚Äô, ‚Ä¶). To be more rigorous and test the validity of the <code class="language-plaintext highlighter-rouge">en_core_web_sm</code> model for SMS, I would have to do spotchecks for more words and drill down whether this might be due to lemmatization or other causes.</p>

<h2 id="summary--conclusions">Summary &amp; Conclusions</h2>

<p>In this project I‚Äôve used basic NLP tools from the <code class="language-plaintext highlighter-rouge">spaCy</code> and <code class="language-plaintext highlighter-rouge">sklearn</code> libraries to study the nature of SMS texts on a public dataset. I hope I‚Äôve convinced you that even basic NLP tools are able to extract insights from SMS data. Here are the main take-aways from this analysis:</p>

<ul>
  <li>
    <p><strong>NLP is able to capture and quantify the short and colloquial nature of SMS:</strong> Through vectorization (using the <code class="language-plaintext highlighter-rouge">nltk.CountVectorizer()</code> and <code class="language-plaintext highlighter-rouge">nltk.TfidfVectorizer()</code> models), the wordclouds show that the most dominant words in the SMS dataset are generic words such as ‚Äòthank‚Äô, ‚Äòknow‚Äô, ‚Äògood‚Äô and colloquial words like ‚Äòlol‚Äô, ‚Äòhaha‚Äô or ‚Äòhi‚Äô.</p>
  </li>
  <li>
    <p><strong>Classical NLP tools don‚Äôt work too well with a variety of languages:</strong> Because most of the available models on <code class="language-plaintext highlighter-rouge">spacy</code> are trained in one language only (English in the case of <code class="language-plaintext highlighter-rouge">en_core_web_sm</code>), it‚Äôs difficult to apply a NLP analysis on SMS dataset from countries that have a rich variety of languages. This is what motivated me to only consider SMS from the US (as opposed to Singapore or India).</p>
  </li>
  <li>
    <p><strong>SMS datasets are extremely sensitive to age bias:</strong> As described in <a href="https://rdcu.be/cR1jY">Chen, T., Kan, MY. Creating a live, public short message service corpus: the NUS SMS corpus</a>, the age distribution of collected SMS is corresponds mostly to an age group of between 18-24 years old (see figure 3 in the paper). The way people write SMS depends a lot on their demographic (i.e. younger people will write SMS differently than younger people) and therefore the results and conclusions from this analysis are most likely biased towards a younger demographic.</p>
  </li>
  <li>
    <p><strong>SMS is not really Natural Language:</strong> Unlike more formal text-based datasets (e.g. movie reviews or policy documents), SMS datasets are extremely loose in their use of language. Some equivalent words might not be captured by the model to be the same becuase they‚Äôre written different (e.g. ‚Äògood‚Äô vs. ‚Äògooood‚Äô) or some abbreviations might not be understood by the model (e.g. ‚Äòlol‚Äô). This will affect the results we get from the NLP analysis since SMS is not really Natural Language in the written sense.</p>
  </li>
</ul>

<h2 id="further-opportunities">Further Opportunities</h2>

<p>I will finish with some final remarks about how one could extend and enhance this analysis:</p>

<ul>
  <li>
    <p><strong>Language Detection:</strong> One interesting question we could ask is: What are the most frequently used words outside of the country‚Äôs official language? For example, it‚Äôd be interesting to study what words (outside of English) are used most frequently in the USA (I‚Äôd expect they‚Äôd be Spanish). We could use one of the many Python libraries to detect language (e.g. <code class="language-plaintext highlighter-rouge">spacy_langdetect</code> or <code class="language-plaintext highlighter-rouge">googletrans</code>), filter out words in the official language(s) of the country, and run a similar vectorisation process as we did here.</p>
  </li>
  <li>
    <p><strong>Alternative stratification/binning:</strong> In this case I binned the data based on the country. I could bin the data based on age, sex or language. The latter would be especially useful given that most available NLP tools in Python cannot handle multilingual datasets. However, age and sex binning wouldn‚Äôt be possible in this dataset because the researchers didn‚Äôt capture that information. Given that language is heavily influenced by demographics, an alternative binning might give us different results and conclusions.</p>
  </li>
  <li>
    <p><strong>Sentiment Analysis:</strong> In this project I used the POS-tagger to get a very basic sentiment for the USA subset of this SMS dataset. We could use more sophisticated ML models for sentiment analysis, such as the <a href="https://ojs.aaai.org/index.php/ICWSM/article/view/14550">VADER model</a>. However, one should be careful to jump on the ML-train as these models are normally trained in datasets of a different domain. Also, we will sacrifice interpretability as these models tend to be a black box.</p>
  </li>
  <li>
    <p><strong>Named Entity Recognition:</strong> In addition to using a POS-tagger, we could run a Named Entity Recognition (NER) analysis, which uses tags words based <a href="https://en.wikipedia.org/wiki/Named_entity">named entities</a> such as location or animal.</p>
  </li>
</ul>

<p>You can find all the code for this analysis <a href="https://github.com/raul-bermejo/sms_nlp_analysis">on this GitHub repo</a>. Stay tuned for part III of this series!</p>
:ET